<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TheOneGIS</title>
  
  <subtitle>空间信息处理技术分享站</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://theonegis.github.io/"/>
  <updated>2019-03-22T19:39:16.792Z</updated>
  <id>http://theonegis.github.io/</id>
  
  <author>
    <name>阿振</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>栅格数据裁剪</title>
    <link href="http://theonegis.github.io/geos/%E6%A0%85%E6%A0%BC%E6%95%B0%E6%8D%AE%E8%A3%81%E5%89%AA/"/>
    <id>http://theonegis.github.io/geos/栅格数据裁剪/</id>
    <published>2019-03-22T07:35:09.000Z</published>
    <updated>2019-03-22T19:39:16.792Z</updated>
    
    <content type="html"><![CDATA[<p>版权声明：本文为博主原创文章，转载请注明原文出处！ 写作时间：2019-03-22</p><p>在进行遥感影像处理的时候，我们经常需要进行裁剪的工作，来看看如何使用GDAL工具进行这项操作吧！</p><p>参考资料：</p><ol type="1"><li><a href="https://www.gdal.org/gdalwarp.html" target="_blank" rel="noopener">GDAL: gdalwarp</a></li><li><a href="https://www.gdal.org/gdal_translate.html" target="_blank" rel="noopener">GDAL: gdal_translate</a></li><li><a href="https://gdal.org/python/" target="_blank" rel="noopener">GDAL/OGR Python API</a></li></ol><h2 id="使用gdal命令">使用GDAL命令</h2><p>GDAL提供了两个命令可以用于影像的裁剪：<code>gdalwarp</code>和<code>gdal_translate</code>，两个命令中我更推荐使用后者。</p><p><code>gdalwarp</code>命令可以使用<code>-te</code>制定裁剪范围。默认是在原数据的坐标系下的<code>xmin ymin xmax ymax</code>，当然我们也可以使用<code>-te_srs</code>参数指定<code>-te</code>参数所在的坐标系。</p><p>为什么不推荐<code>gdalwarp</code>命令呢？这是因为<code>gdalwarp</code>命令只提供了根据坐标系的范围进行裁剪，而不支持根据行列号的裁剪。这时候我们可以求助于<code>gdal_translate</code>命令。</p><p><code>gdal_transalte</code>命令即支持使用<code>-srcwin</code>参数指定行列号范围<code>xoff yoff xsize ysize</code>，也支持使用<code>-projwin</code>参数指定原数据坐标系下的范围<code>ulx uly lrx lry</code>。同时提供参数<code>-projwin_srs</code>可以用于指定<code>-projwin</code>参数所在的坐标系，即跟<code>gdalwarp</code>命令中的<code>-te_srs</code>参数类似。</p><p>下面给出一个示例：</p><p><code>gdal_translate -of &quot;GTiff&quot; -srcwin 10 10 256 256 -a_scale 1 HDF4_EOS:EOS_GRID:&quot;MOD09GA.A2018349.h26v05.006.2018351030314.hdf&quot;:MODIS_Grid_500m_2D:sur_refl_b01_1 sr_1.tif</code></p><p>这行命令将MODIS数据中的反射率的第一波段进行裁剪，起点为第10行第10列，输出大小为256$$255，输出格式为TIFF。</p><p>注意这行命令有一个<code>-a_scale 1</code>参数，这个参数指定了裁剪过程不要对DN值进行缩放。如果不加这个值得话，输出图像的DN值会被根据原数据的<code>Scale=10000</code>放大10000倍。</p><h2 id="使用python代码">使用Python代码</h2><p>对于使用Python代码进行裁剪，我们有两种方法：</p><ul><li>第一就是对命令行对应的借口直接进行调用。这个最直接最简单。</li><li>第二就是首先自己选择出需要裁剪的区域，然后计算裁剪区域的GeoTransform的系数，最后将投影和GeoTransform系数赋值给裁剪子区域，写入输出文件。</li></ul><p>我们知道<a href="https://blog.csdn.net/theonegis/article/details/80304873" target="_blank" rel="noopener">GDAL中使用了六参数模型存储GeoTransform参数</a>，如果进行矩形裁剪的话，只有<code>GT(0)</code>和<code>GT(3)</code>参数会有变化，即需要重新计算裁剪以后的左上角坐标即可。</p><p>下面给出使用Python对MODIS反射率的第一波段进行裁剪的代码：</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">from</span> osgeo <span class="im">import</span> gdal</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb1-3" data-line-number="3"></a><a class="sourceLine" id="cb1-4" data-line-number="4"><span class="co"># API参考：https://gdal.org/python/</span></a><a class="sourceLine" id="cb1-5" data-line-number="5"><span class="co"># GDAL命令行参考：https://www.gdal.org/gdal_translate.html</span></a><a class="sourceLine" id="cb1-6" data-line-number="6">image_name <span class="op">=</span> (<span class="st">&#39;HDF4_EOS:EOS_GRID:&#39;</span></a><a class="sourceLine" id="cb1-7" data-line-number="7">              <span class="st">&#39;&quot;MOD09GA.A2018349.h26v05.006.2018351030314.hdf&quot;:&#39;</span></a><a class="sourceLine" id="cb1-8" data-line-number="8">              <span class="st">&#39;MODIS_Grid_500m_2D:sur_refl_b01_1&#39;</span>)</a><a class="sourceLine" id="cb1-9" data-line-number="9"></a><a class="sourceLine" id="cb1-10" data-line-number="10"><span class="co"># 第一种方式，也是最简单的方法：直接使用GDAL命令行对应的Python方法</span></a><a class="sourceLine" id="cb1-11" data-line-number="11">src: gdal.Dataset <span class="op">=</span> gdal.Open(image_name)</a><a class="sourceLine" id="cb1-12" data-line-number="12">src <span class="op">=</span> gdal.Translate(<span class="st">&#39;cropped_with_translate.tif&#39;</span>, src, srcWin<span class="op">=</span>[<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">256</span>, <span class="dv">256</span>],</a><a class="sourceLine" id="cb1-13" data-line-number="13">                     options<span class="op">=</span>[<span class="st">&#39;-a_scale&#39;</span>, <span class="st">&#39;1&#39;</span>])</a><a class="sourceLine" id="cb1-14" data-line-number="14"><span class="kw">del</span> src</a><a class="sourceLine" id="cb1-15" data-line-number="15"></a><a class="sourceLine" id="cb1-16" data-line-number="16"><span class="co"># 第二种方式，自己选择出需要的像素，然后自己确定裁剪以后的空间参考关系，并写入到输出文件</span></a><a class="sourceLine" id="cb1-17" data-line-number="17">src: gdal.Dataset <span class="op">=</span> gdal.Open(image_name)</a><a class="sourceLine" id="cb1-18" data-line-number="18">band: gdal.Band <span class="op">=</span> src.GetRasterBand(<span class="dv">1</span>)</a><a class="sourceLine" id="cb1-19" data-line-number="19">subset: np.ndarray <span class="op">=</span> band.ReadAsArray(<span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">256</span>, <span class="dv">256</span>)</a><a class="sourceLine" id="cb1-20" data-line-number="20"></a><a class="sourceLine" id="cb1-21" data-line-number="21">driver: gdal.Driver <span class="op">=</span> gdal.GetDriverByName(<span class="st">&#39;GTiff&#39;</span>)</a><a class="sourceLine" id="cb1-22" data-line-number="22">dst: gdal.Dataset <span class="op">=</span> driver.Create(<span class="st">&#39;cropped_from_scratch.tif&#39;</span>, <span class="dv">256</span>, <span class="dv">256</span>, <span class="dv">1</span>, gdal.GDT_Int16)</a><a class="sourceLine" id="cb1-23" data-line-number="23">dst.SetProjection(src.GetProjection())</a><a class="sourceLine" id="cb1-24" data-line-number="24">trans <span class="op">=</span> <span class="bu">list</span>(src.GetGeoTransform())</a><a class="sourceLine" id="cb1-25" data-line-number="25">trans[<span class="dv">0</span>] <span class="op">-=</span> <span class="dv">-10</span> <span class="op">*</span> trans[<span class="dv">1</span>]</a><a class="sourceLine" id="cb1-26" data-line-number="26">trans[<span class="dv">3</span>] <span class="op">-=</span> <span class="dv">-10</span> <span class="op">*</span> trans[<span class="dv">5</span>]</a><a class="sourceLine" id="cb1-27" data-line-number="27">dst.SetGeoTransform(<span class="bu">tuple</span>(trans))</a><a class="sourceLine" id="cb1-28" data-line-number="28"></a><a class="sourceLine" id="cb1-29" data-line-number="29">band: gdal.Band <span class="op">=</span> dst.GetRasterBand(<span class="dv">1</span>)</a><a class="sourceLine" id="cb1-30" data-line-number="30">band.WriteArray(subset)</a><a class="sourceLine" id="cb1-31" data-line-number="31">band.FlushCache()</a><a class="sourceLine" id="cb1-32" data-line-number="32"><span class="kw">del</span> src</a><a class="sourceLine" id="cb1-33" data-line-number="33"><span class="kw">del</span> dst</a></code></pre></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版权声明：本文为博主原创文章，转载请注明原文出处！ 写作时间：2019-03-22&lt;/p&gt;
&lt;p&gt;在进行遥感影像处理的时候，我们经常需要进行裁剪的工作，来看看如何使用GDAL工具进行这项操作吧！&lt;/p&gt;
&lt;p&gt;参考资料：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;a h
      
    
    </summary>
    
      <category term="空间数据处理" scheme="http://theonegis.github.io/categories/geos/"/>
    
    
      <category term="Python" scheme="http://theonegis.github.io/tags/Python/"/>
    
      <category term="GDAL" scheme="http://theonegis.github.io/tags/GDAL/"/>
    
      <category term="裁剪" scheme="http://theonegis.github.io/tags/%E8%A3%81%E5%89%AA/"/>
    
  </entry>
  
  <entry>
    <title>Python中如何优雅地使用switch语句</title>
    <link href="http://theonegis.github.io/python/Python%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E5%9C%B0%E4%BD%BF%E7%94%A8switch%E8%AF%AD%E5%8F%A5/"/>
    <id>http://theonegis.github.io/python/Python中如何优雅地使用switch语句/</id>
    <published>2019-03-07T05:49:45.000Z</published>
    <updated>2019-03-22T19:39:53.825Z</updated>
    
    <content type="html"><![CDATA[<p>版权声明：本文为博主原创文章，转载请注明原文出处！</p><p>写作时间：2019-03-07 13:49:45</p><h1 id="python中如何优雅地使用switch语句">Python中如何优雅地使用switch语句</h1><p>我们知道Python中没有类似C++或者Java中的<code>switch...case</code>语句，我们可以使用多个<code>if...elif...else</code>进行模拟，但是这样的写法让代码看起来很凌乱，个人不是很推荐在代码中大量使用<code>if</code>语句。</p><p>那么解决的办法是什么呢？答曰：字典（<code>dict</code>）。下面我们以两个典型案例进行说明。</p><h2 id="案例一简单情况">案例一（简单情况）</h2><p>第一种简单情况就是一对一，给定一个值，返回一个值，这是C++和Java中的<code>switch</code>语句支持的情况。</p><p>下面的案例是将英文日期翻译为中文日期：</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1">dates <span class="op">=</span> {</a><a class="sourceLine" id="cb1-2" data-line-number="2">    <span class="st">&#39;Sun&#39;</span>: <span class="st">&#39;星期天&#39;</span>, <span class="st">&#39;Mon&#39;</span>: <span class="st">&#39;星期一&#39;</span>, <span class="st">&#39;Tues&#39;</span>: <span class="st">&#39;星期二&#39;</span>, <span class="st">&#39;Wed&#39;</span>: <span class="st">&#39;星期三&#39;</span>,</a><a class="sourceLine" id="cb1-3" data-line-number="3">    <span class="st">&#39;Thurs&#39;</span>: <span class="st">&#39;星期四&#39;</span>, <span class="st">&#39;Fri&#39;</span>: <span class="st">&#39;星期五&#39;</span>, <span class="st">&#39;Sat&#39;</span>: <span class="st">&#39;星期六&#39;</span>}</a><a class="sourceLine" id="cb1-4" data-line-number="4"></a><a class="sourceLine" id="cb1-5" data-line-number="5">day <span class="op">=</span> dates.get(<span class="st">&#39;Fri&#39;</span>, <span class="st">&#39;未知&#39;</span>)</a><a class="sourceLine" id="cb1-6" data-line-number="6"><span class="bu">print</span>(day)  <span class="co"># 输出结果为星期五</span></a></code></pre></div><h2 id="案例二带条件判断">案例二（带条件判断）</h2><p>第二种情况是多对一，反映在编程上就是<code>case</code>语句中带条件判断，这个是诸如Scala中的<code>switch</code>和Kotlin中的<code>when</code>支持的情况。</p><p>下面给出的案例是给定一个数字，如果该数字在某个范围之类，则返回一个指定的数字。</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co"># 这里的conditions是一个函数</span></a><a class="sourceLine" id="cb2-2" data-line-number="2">conditions <span class="op">=</span> <span class="kw">lambda</span> x: {</a><a class="sourceLine" id="cb2-3" data-line-number="3">    x <span class="op">&lt;</span> <span class="dv">-1</span>: <span class="dv">0</span>, <span class="dv">-1</span> <span class="op">&lt;=</span> x <span class="op">&lt;=</span> <span class="dv">1</span>: <span class="fl">0.5</span>, x <span class="op">&gt;</span> <span class="dv">1</span>: <span class="dv">1</span></a><a class="sourceLine" id="cb2-4" data-line-number="4">}</a><a class="sourceLine" id="cb2-5" data-line-number="5"></a><a class="sourceLine" id="cb2-6" data-line-number="6">num <span class="op">=</span> conditions(<span class="fl">0.25</span>)[<span class="va">True</span>]</a><a class="sourceLine" id="cb2-7" data-line-number="7"><span class="bu">print</span>(num)</a><a class="sourceLine" id="cb2-8" data-line-number="8">num <span class="op">=</span> conditions(<span class="dv">10</span>)[<span class="va">True</span>]</a><a class="sourceLine" id="cb2-9" data-line-number="9"><span class="bu">print</span>(num)</a></code></pre></div><p>这里我们的<code>dict</code>不是一个普通的字典，其<code>key</code>是一个<code>lambda</code>表达式（一个函数）。如果我们调用该函数，则会返回一个字典，该字典中有两个元素：一个元素的键是<code>True</code>，另一个是<code>False</code>。<code>True</code>元素包含的值是对应<code>lambda</code>函数中满足条件的给定值，<code>False</code>元素包含的值是对应<code>lambda</code>函数中最后一个不满足条件的给定值（这句话写得比较拗口，不好理解。动手实践一下，可以加深理解）。</p><p>经过上面的介绍，我们以后可以大大减少对<code>if...else</code>语句的使用了，让我们的代码更加干净一些！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版权声明：本文为博主原创文章，转载请注明原文出处！&lt;/p&gt;
&lt;p&gt;写作时间：2019-03-07 13:49:45&lt;/p&gt;
&lt;h1 id=&quot;python中如何优雅地使用switch语句&quot;&gt;Python中如何优雅地使用switch语句&lt;/h1&gt;
&lt;p&gt;我们知道Python中没
      
    
    </summary>
    
      <category term="Python" scheme="http://theonegis.github.io/categories/python/"/>
    
    
      <category term="Python" scheme="http://theonegis.github.io/tags/Python/"/>
    
      <category term="Switch" scheme="http://theonegis.github.io/tags/Switch/"/>
    
  </entry>
  
  <entry>
    <title>使用卷积网络做手写数字识别</title>
    <link href="http://theonegis.github.io/dl/%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%81%9A%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <id>http://theonegis.github.io/dl/使用卷积网络做手写数字识别/</id>
    <published>2019-03-02T14:24:22.000Z</published>
    <updated>2019-03-22T19:39:53.829Z</updated>
    
    <content type="html"><![CDATA[<p>版权声明：本文为博主原创文章，转载请注明原文出处！</p><p>写作时间：2019-03-02 22:24:22</p><h1 id="使用卷积网络做手写数字识别">使用卷积网络做手写数字识别</h1><h2 id="思路分析">思路分析</h2><p>上篇博文《<a href="https://theonegis.blog.csdn.net/article/details/88086423" target="_blank" rel="noopener">使用循环神经网络做手写数字识别</a>》介绍了利用LSTM做手写数字的识别，想着好事成双，也写一个姊妹篇卷积网络实现手写数字的识别。</p><p>博文主要通过最简单的代码量展示一个入门级别的识别案例。需要注意的几点：</p><ul><li>卷积网络的输入大小为（<code>batch_size</code>，<code>num_channels</code>，<code>image_width</code>，<code>image_height</code>）</li><li>本文中的模型使用了卷积层和线性连接层。Linear层的输入大小为（<code>*</code>，<code>num_input_feature</code>），所以在卷积层输出流入线性层的时候，需要转化一下张量的尺寸大小</li><li>综合使用<code>MaxPooling</code>层和<code>Dropout</code>层可以提高识别准确率</li></ul><h2 id="pytorch实现">PyTorch实现</h2><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> torch</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">from</span> torch <span class="im">import</span> nn</a><a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">import</span> torchvision.datasets <span class="im">as</span> datasets</a><a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</a><a class="sourceLine" id="cb1-5" data-line-number="5"></a><a class="sourceLine" id="cb1-6" data-line-number="6">torch.manual_seed(<span class="dv">2019</span>)</a><a class="sourceLine" id="cb1-7" data-line-number="7"></a><a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co"># 超参设置</span></a><a class="sourceLine" id="cb1-9" data-line-number="9">EPOCH <span class="op">=</span> <span class="dv">1</span>  <span class="co"># 训练EPOCH次，这里为了测试方便只跑一次</span></a><a class="sourceLine" id="cb1-10" data-line-number="10">BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></a><a class="sourceLine" id="cb1-11" data-line-number="11">INIT_LR <span class="op">=</span> <span class="fl">1e-3</span>  <span class="co"># 初始学习率</span></a><a class="sourceLine" id="cb1-12" data-line-number="12">DOWNLOAD_MNIST <span class="op">=</span> <span class="va">True</span>  <span class="co"># 设置是否需要下载数据集</span></a><a class="sourceLine" id="cb1-13" data-line-number="13"></a><a class="sourceLine" id="cb1-14" data-line-number="14"><span class="co"># 使用DataLoader加载训练数据，为了演示方便，对于测试数据只取出2000个样本进行测试</span></a><a class="sourceLine" id="cb1-15" data-line-number="15">train_data <span class="op">=</span> datasets.MNIST(root<span class="op">=</span><span class="st">&#39;mnist&#39;</span>, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transforms.ToTensor(), download<span class="op">=</span>DOWNLOAD_MNIST)</a><a class="sourceLine" id="cb1-16" data-line-number="16">train_loader <span class="op">=</span> torch.utils.data.DataLoader(dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb1-17" data-line-number="17">test_data <span class="op">=</span> datasets.MNIST(root<span class="op">=</span><span class="st">&#39;mnist&#39;</span>, train<span class="op">=</span><span class="va">False</span>)</a><a class="sourceLine" id="cb1-18" data-line-number="18">test_x <span class="op">=</span> test_data.test_data.<span class="bu">type</span>(torch.FloatTensor)[:<span class="dv">2000</span>] <span class="op">/</span> <span class="fl">255.</span></a><a class="sourceLine" id="cb1-19" data-line-number="19">test_x.unsqueeze_(<span class="dv">1</span>)  <span class="co"># 调整test_x的尺寸为四维，添加了一个channel维度</span></a><a class="sourceLine" id="cb1-20" data-line-number="20">test_y <span class="op">=</span> test_data.test_labels.numpy()[:<span class="dv">2000</span>]</a><a class="sourceLine" id="cb1-21" data-line-number="21"></a><a class="sourceLine" id="cb1-22" data-line-number="22"></a><a class="sourceLine" id="cb1-23" data-line-number="23"><span class="kw">class</span> ConvNet(nn.Module):</a><a class="sourceLine" id="cb1-24" data-line-number="24">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a><a class="sourceLine" id="cb1-25" data-line-number="25">        <span class="bu">super</span>(ConvNet, <span class="va">self</span>).<span class="fu">__init__</span>()</a><a class="sourceLine" id="cb1-26" data-line-number="26">        <span class="va">self</span>.conv <span class="op">=</span> nn.Sequential(</a><a class="sourceLine" id="cb1-27" data-line-number="27">            nn.Conv2d(<span class="dv">1</span>, <span class="dv">32</span>, <span class="dv">5</span>),  <span class="co"># 图像输出大小为24*24</span></a><a class="sourceLine" id="cb1-28" data-line-number="28">            nn.MaxPool2d(<span class="dv">2</span>),  <span class="co"># 图像输出大小为12*12</span></a><a class="sourceLine" id="cb1-29" data-line-number="29">            nn.ReLU(<span class="va">True</span>),</a><a class="sourceLine" id="cb1-30" data-line-number="30">            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">5</span>),  <span class="co"># 图像输出大小为8*8</span></a><a class="sourceLine" id="cb1-31" data-line-number="31">            nn.Dropout2d(),</a><a class="sourceLine" id="cb1-32" data-line-number="32">            nn.MaxPool2d(<span class="dv">2</span>),  <span class="co"># 图像输出大小为4*4</span></a><a class="sourceLine" id="cb1-33" data-line-number="33">            nn.ReLU(<span class="va">True</span>)</a><a class="sourceLine" id="cb1-34" data-line-number="34">        )</a><a class="sourceLine" id="cb1-35" data-line-number="35"></a><a class="sourceLine" id="cb1-36" data-line-number="36">        <span class="va">self</span>.linear <span class="op">=</span> nn.Sequential(</a><a class="sourceLine" id="cb1-37" data-line-number="37">            nn.Linear(<span class="dv">4</span> <span class="op">*</span> <span class="dv">4</span> <span class="op">*</span> <span class="dv">64</span>, <span class="dv">128</span>),</a><a class="sourceLine" id="cb1-38" data-line-number="38">            nn.ReLU(<span class="va">True</span>),</a><a class="sourceLine" id="cb1-39" data-line-number="39">            nn.Dropout2d(),</a><a class="sourceLine" id="cb1-40" data-line-number="40">            nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>),</a><a class="sourceLine" id="cb1-41" data-line-number="41">            nn.Softmax(<span class="dv">1</span>)</a><a class="sourceLine" id="cb1-42" data-line-number="42">        )</a><a class="sourceLine" id="cb1-43" data-line-number="43"></a><a class="sourceLine" id="cb1-44" data-line-number="44">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a><a class="sourceLine" id="cb1-45" data-line-number="45">        x <span class="op">=</span> <span class="va">self</span>.conv(x)</a><a class="sourceLine" id="cb1-46" data-line-number="46">        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">4</span> <span class="op">*</span> <span class="dv">4</span> <span class="op">*</span> <span class="dv">64</span>)</a><a class="sourceLine" id="cb1-47" data-line-number="47">        out <span class="op">=</span> <span class="va">self</span>.linear(x)</a><a class="sourceLine" id="cb1-48" data-line-number="48">        <span class="cf">return</span> out</a><a class="sourceLine" id="cb1-49" data-line-number="49"></a><a class="sourceLine" id="cb1-50" data-line-number="50"></a><a class="sourceLine" id="cb1-51" data-line-number="51">model <span class="op">=</span> ConvNet()</a><a class="sourceLine" id="cb1-52" data-line-number="52"><span class="bu">print</span>(model)</a><a class="sourceLine" id="cb1-53" data-line-number="53"></a><a class="sourceLine" id="cb1-54" data-line-number="54">optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>INIT_LR)</a><a class="sourceLine" id="cb1-55" data-line-number="55">loss_func <span class="op">=</span> nn.CrossEntropyLoss()</a><a class="sourceLine" id="cb1-56" data-line-number="56"></a><a class="sourceLine" id="cb1-57" data-line-number="57"><span class="co"># RNN训练</span></a><a class="sourceLine" id="cb1-58" data-line-number="58"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCH):</a><a class="sourceLine" id="cb1-59" data-line-number="59">    <span class="cf">for</span> index, (b_x, b_y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</a><a class="sourceLine" id="cb1-60" data-line-number="60">        model.train()</a><a class="sourceLine" id="cb1-61" data-line-number="61">        <span class="co"># 输入尺寸为(batch_size, channels, height, width)</span></a><a class="sourceLine" id="cb1-62" data-line-number="62">        output <span class="op">=</span> model(b_x)  <span class="co"># (64, 1, 28, 28)</span></a><a class="sourceLine" id="cb1-63" data-line-number="63">        loss <span class="op">=</span> loss_func(output, b_y)</a><a class="sourceLine" id="cb1-64" data-line-number="64">        optimizer.zero_grad()</a><a class="sourceLine" id="cb1-65" data-line-number="65">        loss.backward()</a><a class="sourceLine" id="cb1-66" data-line-number="66">        optimizer.step()</a><a class="sourceLine" id="cb1-67" data-line-number="67"></a><a class="sourceLine" id="cb1-68" data-line-number="68">        <span class="cf">if</span> index <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</a><a class="sourceLine" id="cb1-69" data-line-number="69">            model.<span class="bu">eval</span>()</a><a class="sourceLine" id="cb1-70" data-line-number="70">            prediction <span class="op">=</span> model(test_x)  <span class="co"># 输出为(2000, 10)</span></a><a class="sourceLine" id="cb1-71" data-line-number="71">            pred_y <span class="op">=</span> torch.<span class="bu">max</span>(prediction, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()</a><a class="sourceLine" id="cb1-72" data-line-number="72">            accuracy <span class="op">=</span> (pred_y <span class="op">==</span> test_y).<span class="bu">sum</span>() <span class="op">/</span> <span class="bu">float</span>(test_y.size)</a><a class="sourceLine" id="cb1-73" data-line-number="73">            <span class="bu">print</span>(<span class="ss">f&#39;Epoch: [</span><span class="sc">{</span>index<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">]&#39;</span>, <span class="ss">f&#39;| train loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>, <span class="ss">f&#39;| test accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">&#39;</span>)</a><a class="sourceLine" id="cb1-74" data-line-number="74"></a><a class="sourceLine" id="cb1-75" data-line-number="75"><span class="co"># 打印测试数据集中的后20个结果</span></a><a class="sourceLine" id="cb1-76" data-line-number="76">model.<span class="bu">eval</span>()</a><a class="sourceLine" id="cb1-77" data-line-number="77">prediction <span class="op">=</span> model(test_x[:<span class="dv">20</span>])</a><a class="sourceLine" id="cb1-78" data-line-number="78">pred_y <span class="op">=</span> torch.<span class="bu">max</span>(prediction, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()</a><a class="sourceLine" id="cb1-79" data-line-number="79"><span class="bu">print</span>(pred_y, <span class="st">&#39;prediction number&#39;</span>)</a><a class="sourceLine" id="cb1-80" data-line-number="80"><span class="bu">print</span>(test_y[:<span class="dv">20</span>], <span class="st">&#39;real number&#39;</span>)</a></code></pre></div><p>训练结果如下，可以看到对于这种不太复杂的问题，CNN和RNN都可以得到比较高的精度。</p><figure><img src="/images/ml/CNN-MNIST.png" alt="使用卷积网络做手写数字识别"><figcaption>使用卷积网络做手写数字识别</figcaption></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版权声明：本文为博主原创文章，转载请注明原文出处！&lt;/p&gt;
&lt;p&gt;写作时间：2019-03-02 22:24:22&lt;/p&gt;
&lt;h1 id=&quot;使用卷积网络做手写数字识别&quot;&gt;使用卷积网络做手写数字识别&lt;/h1&gt;
&lt;h2 id=&quot;思路分析&quot;&gt;思路分析&lt;/h2&gt;
&lt;p&gt;上篇博文《&lt;
      
    
    </summary>
    
      <category term="深度学习" scheme="http://theonegis.github.io/categories/dl/"/>
    
    
      <category term="PyTorch" scheme="http://theonegis.github.io/tags/PyTorch/"/>
    
      <category term="卷积神经网络" scheme="http://theonegis.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="CNN" scheme="http://theonegis.github.io/tags/CNN/"/>
    
      <category term="手写识别" scheme="http://theonegis.github.io/tags/%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>使用循环神经网络做手写数字识别</title>
    <link href="http://theonegis.github.io/dl/%E4%BD%BF%E7%94%A8%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%81%9A%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
    <id>http://theonegis.github.io/dl/使用循环神经网络做手写数字识别/</id>
    <published>2019-03-02T13:36:12.000Z</published>
    <updated>2019-03-22T19:39:53.830Z</updated>
    
    <content type="html"><![CDATA[<p>版权声明：本文为博主原创文章，转载请注明原文出处！</p><p>写作时间：2019-03-02 21:36:12</p><h1 id="使用循环神经网络做手写数字识别">使用循环神经网络做手写数字识别</h1><h2 id="思路分析">思路分析</h2><p>做图像识别的使用卷积神经网络CNN是最好的选择，但是其实我们也可以使用循环神经网络RNN做，只是大部分时候没有卷积网络效果好！下面分析一下如何使用RNN做手写数字的识别。</p><ol type="1"><li>数据的下载我们可以直接使用PyTorch中的<code>torchvision.datasets</code>提供的数据接口</li><li>对于每一张图像（28$$28）我们可以将图像的每一行看做一个样本，然后所有行排列起来做成一个有序序列。对于这个序列，我们就可以使用RNN做识别训练了。</li><li>下面的实现中使用一个LSTM+Linear层组合实现（不要使用经典RNN，效果不好），损失函数使用CrossEntropyLoss。</li><li>在实践中设置<code>batch_first=True</code>可以减少一些额外的维度变换和尺寸转换的代码，推荐使用</li></ol><h2 id="pytorch实现">PyTorch实现</h2><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> torch</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">from</span> torch <span class="im">import</span> nn</a><a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">import</span> torchvision.datasets <span class="im">as</span> datasets</a><a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</a><a class="sourceLine" id="cb1-5" data-line-number="5"></a><a class="sourceLine" id="cb1-6" data-line-number="6">torch.manual_seed(<span class="dv">2019</span>)</a><a class="sourceLine" id="cb1-7" data-line-number="7"></a><a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co"># 超参设置</span></a><a class="sourceLine" id="cb1-9" data-line-number="9">EPOCH <span class="op">=</span> <span class="dv">1</span>  <span class="co"># 训练EPOCH次，这里为了测试方便只跑一次</span></a><a class="sourceLine" id="cb1-10" data-line-number="10">BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></a><a class="sourceLine" id="cb1-11" data-line-number="11">TIME_STEP <span class="op">=</span> <span class="dv">28</span>  <span class="co"># RNN时间跨度（图片高度）</span></a><a class="sourceLine" id="cb1-12" data-line-number="12">INPUT_SIZE <span class="op">=</span> <span class="dv">28</span>  <span class="co"># RNN输入尺寸（图片宽度）</span></a><a class="sourceLine" id="cb1-13" data-line-number="13">INIT_LR <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># 初始学习率</span></a><a class="sourceLine" id="cb1-14" data-line-number="14">DOWNLOAD_MNIST <span class="op">=</span> <span class="va">True</span>  <span class="co"># 设置是否需要下载数据集</span></a><a class="sourceLine" id="cb1-15" data-line-number="15"></a><a class="sourceLine" id="cb1-16" data-line-number="16"><span class="co"># 使用DataLoader加载训练数据，为了演示方便，对于测试数据只取出2000个样本进行测试</span></a><a class="sourceLine" id="cb1-17" data-line-number="17">train_data <span class="op">=</span> datasets.MNIST(root<span class="op">=</span><span class="st">&#39;mnist&#39;</span>, train<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transforms.ToTensor(), download<span class="op">=</span>DOWNLOAD_MNIST)</a><a class="sourceLine" id="cb1-18" data-line-number="18">train_loader <span class="op">=</span> torch.utils.data.DataLoader(dataset<span class="op">=</span>train_data, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb1-19" data-line-number="19">test_data <span class="op">=</span> datasets.MNIST(root<span class="op">=</span><span class="st">&#39;mnist&#39;</span>, train<span class="op">=</span><span class="va">False</span>)</a><a class="sourceLine" id="cb1-20" data-line-number="20">test_x <span class="op">=</span> test_data.test_data.<span class="bu">type</span>(torch.FloatTensor)[:<span class="dv">2000</span>] <span class="op">/</span> <span class="fl">255.</span></a><a class="sourceLine" id="cb1-21" data-line-number="21">test_y <span class="op">=</span> test_data.test_labels.numpy()[:<span class="dv">2000</span>]</a><a class="sourceLine" id="cb1-22" data-line-number="22"></a><a class="sourceLine" id="cb1-23" data-line-number="23"></a><a class="sourceLine" id="cb1-24" data-line-number="24"><span class="kw">class</span> RNN(nn.Module):</a><a class="sourceLine" id="cb1-25" data-line-number="25">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a><a class="sourceLine" id="cb1-26" data-line-number="26">        <span class="bu">super</span>(RNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a><a class="sourceLine" id="cb1-27" data-line-number="27">        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(</a><a class="sourceLine" id="cb1-28" data-line-number="28">            input_size<span class="op">=</span>INPUT_SIZE,</a><a class="sourceLine" id="cb1-29" data-line-number="29">            hidden_size<span class="op">=</span><span class="dv">64</span>,</a><a class="sourceLine" id="cb1-30" data-line-number="30">            num_layers<span class="op">=</span><span class="dv">1</span>,</a><a class="sourceLine" id="cb1-31" data-line-number="31">            batch_first<span class="op">=</span><span class="va">True</span></a><a class="sourceLine" id="cb1-32" data-line-number="32">        )</a><a class="sourceLine" id="cb1-33" data-line-number="33">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">64</span>, <span class="dv">10</span>)</a><a class="sourceLine" id="cb1-34" data-line-number="34"></a><a class="sourceLine" id="cb1-35" data-line-number="35">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a><a class="sourceLine" id="cb1-36" data-line-number="36">        <span class="co"># x shape (batch_size, time_step, input_size)</span></a><a class="sourceLine" id="cb1-37" data-line-number="37">        <span class="co"># r_out shape (batch_size, time_step, output_size)</span></a><a class="sourceLine" id="cb1-38" data-line-number="38">        <span class="co"># h_n shape (n_layers, batch_size, hidden_size)</span></a><a class="sourceLine" id="cb1-39" data-line-number="39">        <span class="co"># h_c shape (n_layers, batch_size, hidden_size)</span></a><a class="sourceLine" id="cb1-40" data-line-number="40">        r_out, (h_n, h_c) <span class="op">=</span> <span class="va">self</span>.rnn(x)</a><a class="sourceLine" id="cb1-41" data-line-number="41">        <span class="co"># 取出最后一次循环的r_out传递到全连接层</span></a><a class="sourceLine" id="cb1-42" data-line-number="42">        out <span class="op">=</span> <span class="va">self</span>.out(r_out[:, <span class="dv">-1</span>, :])</a><a class="sourceLine" id="cb1-43" data-line-number="43">        <span class="cf">return</span> out</a><a class="sourceLine" id="cb1-44" data-line-number="44"></a><a class="sourceLine" id="cb1-45" data-line-number="45"></a><a class="sourceLine" id="cb1-46" data-line-number="46">rnn <span class="op">=</span> RNN()</a><a class="sourceLine" id="cb1-47" data-line-number="47"><span class="bu">print</span>(rnn)</a><a class="sourceLine" id="cb1-48" data-line-number="48"></a><a class="sourceLine" id="cb1-49" data-line-number="49">optimizer <span class="op">=</span> torch.optim.Adam(rnn.parameters(), lr<span class="op">=</span>INIT_LR)</a><a class="sourceLine" id="cb1-50" data-line-number="50">loss_func <span class="op">=</span> nn.CrossEntropyLoss()</a><a class="sourceLine" id="cb1-51" data-line-number="51"></a><a class="sourceLine" id="cb1-52" data-line-number="52"><span class="co"># RNN训练</span></a><a class="sourceLine" id="cb1-53" data-line-number="53"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCH):</a><a class="sourceLine" id="cb1-54" data-line-number="54">    <span class="cf">for</span> step, (b_x, b_y) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</a><a class="sourceLine" id="cb1-55" data-line-number="55">        <span class="co"># 数据的输入为(batch_size, time_step, input_size)</span></a><a class="sourceLine" id="cb1-56" data-line-number="56">        b_x <span class="op">=</span> b_x.view(<span class="op">-</span><span class="dv">1</span>, TIME_STEP, INPUT_SIZE)</a><a class="sourceLine" id="cb1-57" data-line-number="57">        output <span class="op">=</span> rnn(b_x)</a><a class="sourceLine" id="cb1-58" data-line-number="58">        loss <span class="op">=</span> loss_func(output, b_y)</a><a class="sourceLine" id="cb1-59" data-line-number="59">        optimizer.zero_grad()</a><a class="sourceLine" id="cb1-60" data-line-number="60">        loss.backward()</a><a class="sourceLine" id="cb1-61" data-line-number="61">        optimizer.step()</a><a class="sourceLine" id="cb1-62" data-line-number="62"></a><a class="sourceLine" id="cb1-63" data-line-number="63">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</a><a class="sourceLine" id="cb1-64" data-line-number="64">            prediction <span class="op">=</span> rnn(test_x)  <span class="co"># 输出为(2000, 10)</span></a><a class="sourceLine" id="cb1-65" data-line-number="65">            pred_y <span class="op">=</span> torch.<span class="bu">max</span>(prediction, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()</a><a class="sourceLine" id="cb1-66" data-line-number="66">            accuracy <span class="op">=</span> (pred_y <span class="op">==</span> test_y).<span class="bu">sum</span>() <span class="op">/</span> <span class="bu">float</span>(test_y.size)</a><a class="sourceLine" id="cb1-67" data-line-number="67">            <span class="bu">print</span>(<span class="ss">f&#39;Epoch: [</span><span class="sc">{</span>step<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">]&#39;</span>, <span class="ss">f&#39;| train loss: </span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">}</span><span class="ss">&#39;</span>, <span class="ss">f&#39;| test accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">&#39;</span>)</a><a class="sourceLine" id="cb1-68" data-line-number="68"></a><a class="sourceLine" id="cb1-69" data-line-number="69"><span class="co"># 打印测试数据集中的后20个结果</span></a><a class="sourceLine" id="cb1-70" data-line-number="70">prediction <span class="op">=</span> rnn(test_x[:<span class="dv">20</span>].view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>))</a><a class="sourceLine" id="cb1-71" data-line-number="71">pred_y <span class="op">=</span> torch.<span class="bu">max</span>(prediction, <span class="dv">1</span>)[<span class="dv">1</span>].data.numpy()</a><a class="sourceLine" id="cb1-72" data-line-number="72"><span class="bu">print</span>(pred_y, <span class="st">&#39;prediction number&#39;</span>)</a><a class="sourceLine" id="cb1-73" data-line-number="73"><span class="bu">print</span>(test_y[:<span class="dv">20</span>], <span class="st">&#39;real number&#39;</span>)</a></code></pre></div><p>下面是训练结果的截图，可以看到效果还不错！</p><figure><img src="/images/ml/LSTM-MNIST.png" alt="使用循环神经网络做手写数字识别"><figcaption>使用循环神经网络做手写数字识别</figcaption></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版权声明：本文为博主原创文章，转载请注明原文出处！&lt;/p&gt;
&lt;p&gt;写作时间：2019-03-02 21:36:12&lt;/p&gt;
&lt;h1 id=&quot;使用循环神经网络做手写数字识别&quot;&gt;使用循环神经网络做手写数字识别&lt;/h1&gt;
&lt;h2 id=&quot;思路分析&quot;&gt;思路分析&lt;/h2&gt;
&lt;p&gt;做图
      
    
    </summary>
    
      <category term="深度学习" scheme="http://theonegis.github.io/categories/dl/"/>
    
    
      <category term="PyTorch" scheme="http://theonegis.github.io/tags/PyTorch/"/>
    
      <category term="手写识别" scheme="http://theonegis.github.io/tags/%E6%89%8B%E5%86%99%E8%AF%86%E5%88%AB/"/>
    
      <category term="循环神经网络" scheme="http://theonegis.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="RNN" scheme="http://theonegis.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>通俗LSTM长短时循环神经网络介绍</title>
    <link href="http://theonegis.github.io/dl/%E9%80%9A%E4%BF%97LSTM%E9%95%BF%E7%9F%AD%E6%97%B6%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>http://theonegis.github.io/dl/通俗LSTM长短时循环神经网络介绍/</id>
    <published>2019-03-02T10:20:11.000Z</published>
    <updated>2019-03-22T19:39:53.836Z</updated>
    
    <content type="html"><![CDATA[<p>版权声明：本文为博主原创文章，转载请注明原文出处！ 写作时间：2019-03-02 18:20:11 本文部分图片素材来自互联网，如有侵权，请联系作者删除！</p><h1 id="通俗lstm长短时记忆循环神经网络介绍">通俗LSTM长短时记忆循环神经网络介绍</h1><h2 id="lstm图解">LSTM图解</h2><h3 id="处理流程">处理流程</h3><p>在上一篇<a href="https://blog.csdn.net/theonegis/article/details/88084305" target="_blank" rel="noopener">文章</a>中简单介绍了经典RNN模型，并提到了RNN的一些缺点。LSTM（Long Short-Term Memory）解决了经典RNN不能很好地保存长时序信息的缺点，得到了更加广泛地应用。下面简单说说LSTM的流程。</p><figure><img src="/images/ml/Long_Short-Term_Memory.png" alt="Long Short-Term Memory"><figcaption>Long Short-Term Memory</figcaption></figure><p>通过对比我们可以发现，LSTM和经典RNN有如下的区别：</p><ul><li>除了中间状态H，还多了一个C</li><li>每个循环网络的单元（Cell）变得复杂了（多了所谓的三道门“遗忘门”（forget gate），“输入门”（input gate）和“输出门”（output gate））</li></ul><p>这里所谓的“门”其实就是选择性地对信息进行过滤，在实践中用<code>sigmoid</code>函数（在图中用<span class="math inline">\(\sigma\)</span>表示）实现。</p><p>首先，<span class="math inline">\(t-1\)</span>时刻的输入<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(x_t\)</span>经过一个线性变换+<code>sigmoid</code>激活以后（这就是所谓的遗忘门），输出<span class="math inline">\(f_t\)</span>。<span class="math inline">\(f_t\)</span>再与<span class="math inline">\(c_{t-1}\)</span>进行相乘（element-wise multiplication）得到一个中间结果。</p><p>然后，<span class="math inline">\(t-1\)</span>时刻的输入<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(x_t\)</span>经过另外一个线性变换+<code>sigmoid</code>激活以后（这就是所谓的输入门），输出<span class="math inline">\(l_t\)</span>。同时，<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(x_t\)</span>经过再另外一个线性变换+<code>tanh</code>激活以后），与<span class="math inline">\(l_t\)</span>相乘得到一个中间结果。这个中间结果和上一步的中间结果相加（element-wise addition）得到<span class="math inline">\(c_t\)</span>。</p><p>最后，<span class="math inline">\(t-1\)</span>时刻的输入<span class="math inline">\(h_{t-1}\)</span>和<span class="math inline">\(x_t\)</span>经过另外一个线性变换+<code>sigmoid</code>激活以后（这就是所谓的输出门），输出<span class="math inline">\(o_t\)</span>。<span class="math inline">\(o_t\)</span>与经过<code>tanh</code>的<span class="math inline">\(c_t\)</span>相乘得到<span class="math inline">\(h_t\)</span>。</p><p>至此，所有的状态更新完毕。</p><h3 id="流程图解">流程图解</h3><p>下面给出上面文字描述的步骤所对应的数学公式：</p><figure><img src="/images/ml/LSTM3-focus-f.png" alt="LSTM第一步遗忘门"><figcaption>LSTM第一步遗忘门</figcaption></figure><figure><img src="/images/ml/LSTM3-focus-i.png" alt="LSTM第二步输入门"><figcaption>LSTM第二步输入门</figcaption></figure><figure><img src="/images/ml/LSTM3-focus-C.png" alt="LSTM得到中间状态C"><figcaption>LSTM得到中间状态C</figcaption></figure><figure><img src="/images/ml/LSTM3-focus-o.png" alt="LSTM第三步输出门"><figcaption>LSTM第三步输出门</figcaption></figure><h3 id="总结说明">总结说明</h3><figure><img src="/images/ml/LSTM-Pipeline.png" alt="LSTM数据管道"><figcaption>LSTM数据管道</figcaption></figure><p>上图的左子图给出了对于每个门的输入和输出，右子图说明了每个门的作用。</p><h2 id="pytorch实战">PyTorch实战</h2><p>我们还是以《<a href="https://blog.csdn.net/theonegis" target="_blank" rel="noopener">最简单的RNN回归模型入门</a>》中的使用Sin预测Cos的例子进行演示，代码跟之间的没有太大的区别，唯一的不同就是在中间状态更新的时候，现在有C和H两种中间状态需要更新。</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> torch</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">from</span> torch <span class="im">import</span> nn</a><a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a><a class="sourceLine" id="cb1-5" data-line-number="5"></a><a class="sourceLine" id="cb1-6" data-line-number="6">torch.manual_seed(<span class="dv">2019</span>)</a><a class="sourceLine" id="cb1-7" data-line-number="7"></a><a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co"># 超参设置</span></a><a class="sourceLine" id="cb1-9" data-line-number="9">TIME_STEP <span class="op">=</span> <span class="dv">20</span>  <span class="co"># RNN时间步长</span></a><a class="sourceLine" id="cb1-10" data-line-number="10">INPUT_SIZE <span class="op">=</span> <span class="dv">1</span>  <span class="co"># RNN输入尺寸</span></a><a class="sourceLine" id="cb1-11" data-line-number="11">INIT_LR <span class="op">=</span> <span class="fl">0.02</span>  <span class="co"># 初始学习率</span></a><a class="sourceLine" id="cb1-12" data-line-number="12">N_EPOCHS <span class="op">=</span> <span class="dv">100</span>  <span class="co"># 训练回数</span></a><a class="sourceLine" id="cb1-13" data-line-number="13"></a><a class="sourceLine" id="cb1-14" data-line-number="14"></a><a class="sourceLine" id="cb1-15" data-line-number="15"><span class="kw">class</span> RNN(nn.Module):</a><a class="sourceLine" id="cb1-16" data-line-number="16">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a><a class="sourceLine" id="cb1-17" data-line-number="17">        <span class="bu">super</span>(RNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a><a class="sourceLine" id="cb1-18" data-line-number="18">        <span class="va">self</span>.rnn <span class="op">=</span> nn.LSTM(</a><a class="sourceLine" id="cb1-19" data-line-number="19">            input_size<span class="op">=</span>INPUT_SIZE,</a><a class="sourceLine" id="cb1-20" data-line-number="20">            hidden_size<span class="op">=</span><span class="dv">32</span>,  <span class="co"># RNN隐藏神经元个数</span></a><a class="sourceLine" id="cb1-21" data-line-number="21">            num_layers<span class="op">=</span><span class="dv">1</span>,  <span class="co"># RNN隐藏层个数</span></a><a class="sourceLine" id="cb1-22" data-line-number="22">        )</a><a class="sourceLine" id="cb1-23" data-line-number="23">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">1</span>)</a><a class="sourceLine" id="cb1-24" data-line-number="24"></a><a class="sourceLine" id="cb1-25" data-line-number="25">    <span class="kw">def</span> forward(<span class="va">self</span>, x, h):</a><a class="sourceLine" id="cb1-26" data-line-number="26">        <span class="co"># x (time_step, batch_size, input_size)</span></a><a class="sourceLine" id="cb1-27" data-line-number="27">        <span class="co"># h (n_layers, batch, hidden_size)</span></a><a class="sourceLine" id="cb1-28" data-line-number="28">        <span class="co"># out (time_step, batch_size, hidden_size)</span></a><a class="sourceLine" id="cb1-29" data-line-number="29">        out, h <span class="op">=</span> <span class="va">self</span>.rnn(x, h)</a><a class="sourceLine" id="cb1-30" data-line-number="30">        prediction <span class="op">=</span> <span class="va">self</span>.out(out)</a><a class="sourceLine" id="cb1-31" data-line-number="31">        <span class="cf">return</span> prediction, h</a><a class="sourceLine" id="cb1-32" data-line-number="32"></a><a class="sourceLine" id="cb1-33" data-line-number="33"></a><a class="sourceLine" id="cb1-34" data-line-number="34">rnn <span class="op">=</span> RNN()</a><a class="sourceLine" id="cb1-35" data-line-number="35"><span class="bu">print</span>(rnn)</a><a class="sourceLine" id="cb1-36" data-line-number="36"></a><a class="sourceLine" id="cb1-37" data-line-number="37">optimizer <span class="op">=</span> torch.optim.Adam(rnn.parameters(), lr<span class="op">=</span>INIT_LR)</a><a class="sourceLine" id="cb1-38" data-line-number="38">loss_func <span class="op">=</span> nn.MSELoss()</a><a class="sourceLine" id="cb1-39" data-line-number="39">h_state <span class="op">=</span> <span class="va">None</span>  <span class="co"># 初始化隐藏层</span></a><a class="sourceLine" id="cb1-40" data-line-number="40"></a><a class="sourceLine" id="cb1-41" data-line-number="41">plt.figure()</a><a class="sourceLine" id="cb1-42" data-line-number="42">plt.ion()</a><a class="sourceLine" id="cb1-43" data-line-number="43"><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(N_EPOCHS):</a><a class="sourceLine" id="cb1-44" data-line-number="44">    start, end <span class="op">=</span> step <span class="op">*</span> np.pi, (step <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> np.pi  <span class="co"># 时间跨度</span></a><a class="sourceLine" id="cb1-45" data-line-number="45">    <span class="co"># 使用Sin函数预测Cos函数</span></a><a class="sourceLine" id="cb1-46" data-line-number="46">    steps <span class="op">=</span> np.linspace(start, end, TIME_STEP, dtype<span class="op">=</span>np.float32, endpoint<span class="op">=</span><span class="va">False</span>)</a><a class="sourceLine" id="cb1-47" data-line-number="47">    x_np <span class="op">=</span> np.sin(steps)</a><a class="sourceLine" id="cb1-48" data-line-number="48">    y_np <span class="op">=</span> np.cos(steps)</a><a class="sourceLine" id="cb1-49" data-line-number="49">    x <span class="op">=</span> torch.from_numpy(x_np[:, np.newaxis, np.newaxis])  <span class="co"># 尺寸大小为(time_step, batch, input_size)</span></a><a class="sourceLine" id="cb1-50" data-line-number="50">    y <span class="op">=</span> torch.from_numpy(y_np[:, np.newaxis, np.newaxis])</a><a class="sourceLine" id="cb1-51" data-line-number="51"></a><a class="sourceLine" id="cb1-52" data-line-number="52">    prediction, h_state <span class="op">=</span> rnn(x, h_state)  <span class="co"># RNN输出（预测结果，隐藏状态）</span></a><a class="sourceLine" id="cb1-53" data-line-number="53">    h_state <span class="op">=</span> (h_state[<span class="dv">0</span>].detach(), h_state[<span class="dv">1</span>].detach())  <span class="co"># 注意这里和原来的RNN的不同</span></a><a class="sourceLine" id="cb1-54" data-line-number="54">    loss <span class="op">=</span> loss_func(prediction, y)</a><a class="sourceLine" id="cb1-55" data-line-number="55">    optimizer.zero_grad()</a><a class="sourceLine" id="cb1-56" data-line-number="56">    loss.backward()</a><a class="sourceLine" id="cb1-57" data-line-number="57">    optimizer.step()</a><a class="sourceLine" id="cb1-58" data-line-number="58"></a><a class="sourceLine" id="cb1-59" data-line-number="59">    <span class="co"># 绘制中间结果</span></a><a class="sourceLine" id="cb1-60" data-line-number="60">    plt.cla()</a><a class="sourceLine" id="cb1-61" data-line-number="61">    plt.plot(steps, y_np, <span class="st">&#39;r-&#39;</span>)</a><a class="sourceLine" id="cb1-62" data-line-number="62">    plt.plot(steps, prediction.data.numpy().flatten(), <span class="st">&#39;b-&#39;</span>)</a><a class="sourceLine" id="cb1-63" data-line-number="63">    plt.draw()</a><a class="sourceLine" id="cb1-64" data-line-number="64">    plt.pause(<span class="fl">0.1</span>)</a><a class="sourceLine" id="cb1-65" data-line-number="65">plt.ioff()</a><a class="sourceLine" id="cb1-66" data-line-number="66">plt.show()</a></code></pre></div><p>当<code>TIME_STEP</code>设置为20的时候，输出结果如下：</p><figure><img src="/images/ml/LSTM-Sin-20.png" alt="LSTM Sin预测Cos"><figcaption>LSTM Sin预测Cos</figcaption></figure><h2 id="参考资料">参考资料</h2><ol type="1"><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li><li><a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" target="_blank" rel="noopener">Understanding LSTM and its diagrams</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版权声明：本文为博主原创文章，转载请注明原文出处！ 写作时间：2019-03-02 18:20:11 本文部分图片素材来自互联网，如有侵权，请联系作者删除！&lt;/p&gt;
&lt;h1 id=&quot;通俗lstm长短时记忆循环神经网络介绍&quot;&gt;通俗LSTM长短时记忆循环神经网络介绍&lt;/h1&gt;

      
    
    </summary>
    
      <category term="深度学习" scheme="http://theonegis.github.io/categories/dl/"/>
    
    
      <category term="循环神经网络" scheme="http://theonegis.github.io/tags/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="长短时" scheme="http://theonegis.github.io/tags/%E9%95%BF%E7%9F%AD%E6%97%B6/"/>
    
  </entry>
  
  <entry>
    <title>最简单的RNN回归模型入门(PyTorch)</title>
    <link href="http://theonegis.github.io/dl/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84RNN%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E5%85%A5%E9%97%A8-PyTorch/"/>
    <id>http://theonegis.github.io/dl/最简单的RNN回归模型入门-PyTorch/</id>
    <published>2019-03-02T04:46:15.000Z</published>
    <updated>2019-03-22T19:39:53.834Z</updated>
    
    <content type="html"><![CDATA[<p>版权声明：本文为博主原创文章，转载请注明原文出处！ 写作时间：2019-03-02 12:46:15</p><p>本文部分图片素材来自互联网，如有侵权，请联系作者删除！</p><h1 id="最简单的rnn回归模型入门pytorch版">最简单的RNN回归模型入门（PyTorch版）</h1><h2 id="rnn入门介绍">RNN入门介绍</h2><p>至于RNN的能做什么，擅长什么，这里不赘述。如果不清楚，请先维基一下，那里比我说得更加清楚。</p><p>我们首先来来看一张经典的RNN模型示意图！</p><figure><img src="/images/ml/Recurrent-Neural-Network.png" alt="Recurrent Neural Network"><figcaption>Recurrent Neural Network</figcaption></figure><p>图分左右两边：左边给出的RNN是一个抽象的循环结构，右边是左边RNN展开以后的形式。先来看右边的结构，从下往上依次是序列数据的输入X（图中的绿色结构，可以是时间序列，也可以是文本序列等等）。对于t时刻的x经过一个线性变换（U是变换的权重），然后与t-1时刻经过线性变换V的h相加，再经过一个 非线性激活（一般使用tanh或者relu函数）以后，形成一个t时刻的中间状态h，然后再经过一个线性变换（W）输出o ，最后再经过一个非线性激活（可以是sigmoid函数或者softmax等函数）形成最后的输出y。</p><p>上面的文字描述，可以形式化表示为下面的公式：</p><p><span class="math display">\[a^t = Vh^{t-1} + Ux^t + b \\ h^t=tanh(a^t) \\ o^t=Wh^t + c\\ y^t=sigmoid(o^t)\]</span></p><p>是不是公式能比文字更加说明问题！</p><p>再来说左边的结构，坐标的结构表明后面地展开网络中的U，V，W参数都是在共享的，就是说不管我们的序列有多长，都是共享这一套参数的。这是RNN很重要的一个特性。</p><p>RNN的隐藏层可以有多层，但是RNN中我们的隐藏层一般不会设置太多，因为在横向上有很长的序列扩展形成的网络，这部分特征是我们更加关注的。最后，需要说明的是RNN可以是单向的，也可以是双向的。</p><h2 id="pytorch中的rnn">PyTorch中的RNN</h2><p>下面我们以一个最简单的回归问题使用正弦sin函数预测余弦cos函数，介绍如何使用PyTorch实现RNN模型。</p><p>先来看一下PyTorch中<a href="https://pytorch.org/docs/stable/nn.html#rnn" target="_blank" rel="noopener">RNN</a>类的原型：</p><figure><img src="/images/ml/RNNClass.png" alt="torch.nn.RNN"><figcaption>torch.nn.RNN</figcaption></figure><ul><li>必选参数<code>input_size</code>指定输入序列中单个样本的大小尺寸，比如在NLP中我们可能用用一个10000个长度的向量表示一个单词，则这个<code>input_size</code>就是10000。在咱们的回归案例中，一个序列中包含若干点，而每个点的所代表的函数值（Y）作为一个样本，则咱们案例中的<code>input_size</code>为1。这个参数需要根据自己的实际问题确定。</li><li>必选参数<code>hidden_size</code>指的是隐藏层中输出特征的大小，这个是自定义的超参数。</li><li>必选参数<code>num_layers</code>指的是纵向的隐藏层的个数，根据实际问题我们一般可以选择1~10层。</li><li>可选参数<code>batch_first</code>指定是否将<code>batch_size</code>作为输入输出张量的第一个维度，如果是，则输入的尺寸为（<code>batch_size</code>， <code>seq_length</code>，<code>input_size</code>），否则，默认的顺序是（<code>seq_length</code>，<code>batch_size</code>， <code>input_size</code>）。</li><li>可选参数<code>bidirectional</code>指定是否使用双向RNN。</li></ul><p>下面再来说说RNN输入输出尺寸的问题，了解了这个可以让我们我们调试代码的时候更加清晰。下面是PyTorch官方的说明：</p><figure><img src="/images/ml/RNNInOut.png" alt="RNN的输入输出"><figcaption>RNN的输入输出</figcaption></figure><p>对于RNN的输入包括输入序列和一个初始化的隐藏状态<span class="math inline">\(h_0\)</span>。输入序列尺寸默认是（<code>sequence_length</code>，<code>batch_size</code>， <code>input_size</code>），所以如果我们的数据形式不是这样的，则需要手动调整为这种类型的格式。</p><p>隐藏状态<span class="math inline">\(h_i\)</span>的尺寸是（<code>num_layers * num_directions</code>， <code>batch_size</code>，<code>hidden_size</code>）。单向RNN的<code>num_directions</code>为1，双向RNN的<code>num_directions</code>为2。</p><p>他们的尺寸为什么是这样的呢？这得根据本文开头的那个公式计算，即就是矩阵的相乘需要满足矩阵尺寸的关系，聪明的你想明白了吗？</p><p>输出的尺寸为 （<code>sequence_length</code>， <code>batch_size</code>， <code>num_directions * hidden_size</code>）</p><p>每一次RNN运行结果输出中还会附带输出中间隐藏状态<span class="math inline">\(h_i\)</span>，当然这个尺寸和初始的隐藏状态相同。</p><p>下面以一个简单的例子说明怎么在程序中查看他们的尺寸：</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> torch</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> torch.nn <span class="im">as</span> nn</a><a class="sourceLine" id="cb1-3" data-line-number="3"></a><a class="sourceLine" id="cb1-4" data-line-number="4">rnn <span class="op">=</span> nn.RNN(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">2</span>)</a><a class="sourceLine" id="cb1-5" data-line-number="5">inputs <span class="op">=</span> torch.randn(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">10</span>)  <span class="co"># (time_step, batch_size, input_size)</span></a><a class="sourceLine" id="cb1-6" data-line-number="6">h0 <span class="op">=</span> torch.randn(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">20</span>)  <span class="co"># (num_layers, batch_size, hidden_size)</span></a><a class="sourceLine" id="cb1-7" data-line-number="7">output, hn <span class="op">=</span> rnn(inputs, h0)</a><a class="sourceLine" id="cb1-8" data-line-number="8"><span class="bu">print</span>(output.shape)  <span class="co"># (time_step, batch_size, hidden_size)</span></a><a class="sourceLine" id="cb1-9" data-line-number="9"></a><a class="sourceLine" id="cb1-10" data-line-number="10"><span class="cf">for</span> name, param <span class="kw">in</span> rnn.named_parameters():</a><a class="sourceLine" id="cb1-11" data-line-number="11">    <span class="cf">if</span> param.requires_grad:</a><a class="sourceLine" id="cb1-12" data-line-number="12">        <span class="bu">print</span>(name, param.size())</a></code></pre></div><p>其输出结果如下：</p><pre><code>torch.Size([5, 3, 20])weight_ih_l0 torch.Size([20, 10])weight_hh_l0 torch.Size([20, 20])bias_ih_l0 torch.Size([20])bias_hh_l0 torch.Size([20])weight_ih_l1 torch.Size([20, 20])weight_hh_l1 torch.Size([20, 20])bias_ih_l1 torch.Size([20])bias_hh_l1 torch.Size([20])</code></pre><p>这里的<code>weight_ih_l0</code>表示的是RNN隐藏层第一层的权重U，<code>weight_hh_l0</code>表示的隐藏层第一层的权重V，类似的<code>bias</code>开头的表示偏置或者叫增益（我不知道中文如何翻译），以<code>l数字</code>结尾的表示第几层的权重或者偏置。</p><h2 id="代码实现与结果分析">代码实现与结果分析</h2><p>好了，搞清楚了RNN的基本原理以及PyTorch中RNN类的输入输出参数要求，我们下面实现我们的回归案例。</p><p>比较重要的几个超参数是：<code>TIME_STEP</code>指定输入序列的长度（一个序列中包含的函数值的个数），<code>INPUT_SIZE</code>是1，表示一个序列中的每个样本包含一个函数值。</p><p>我们自定义的RNN类包含两个模型：一个<code>nn.RNN</code>层，一个<code>nn.Linear</code>层，注意<code>forward</code>函数的实现，观察每个变量的尺寸（注释中给出了答案）。</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">import</span> torch</a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">from</span> torch <span class="im">import</span> nn</a><a class="sourceLine" id="cb3-3" data-line-number="3"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb3-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a><a class="sourceLine" id="cb3-5" data-line-number="5"></a><a class="sourceLine" id="cb3-6" data-line-number="6">torch.manual_seed(<span class="dv">2019</span>)</a><a class="sourceLine" id="cb3-7" data-line-number="7"></a><a class="sourceLine" id="cb3-8" data-line-number="8"><span class="co"># 超参设置</span></a><a class="sourceLine" id="cb3-9" data-line-number="9">TIME_STEP <span class="op">=</span> <span class="dv">10</span>  <span class="co"># RNN时间步长</span></a><a class="sourceLine" id="cb3-10" data-line-number="10">INPUT_SIZE <span class="op">=</span> <span class="dv">1</span>  <span class="co"># RNN输入尺寸</span></a><a class="sourceLine" id="cb3-11" data-line-number="11">INIT_LR <span class="op">=</span> <span class="fl">0.02</span>  <span class="co"># 初始学习率</span></a><a class="sourceLine" id="cb3-12" data-line-number="12">N_EPOCHS <span class="op">=</span> <span class="dv">100</span>  <span class="co"># 训练回数</span></a><a class="sourceLine" id="cb3-13" data-line-number="13"></a><a class="sourceLine" id="cb3-14" data-line-number="14"></a><a class="sourceLine" id="cb3-15" data-line-number="15"><span class="kw">class</span> RNN(nn.Module):</a><a class="sourceLine" id="cb3-16" data-line-number="16">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</a><a class="sourceLine" id="cb3-17" data-line-number="17">        <span class="bu">super</span>(RNN, <span class="va">self</span>).<span class="fu">__init__</span>()</a><a class="sourceLine" id="cb3-18" data-line-number="18">        <span class="va">self</span>.rnn <span class="op">=</span> nn.RNN(</a><a class="sourceLine" id="cb3-19" data-line-number="19">            input_size<span class="op">=</span>INPUT_SIZE,</a><a class="sourceLine" id="cb3-20" data-line-number="20">            hidden_size<span class="op">=</span><span class="dv">32</span>,  <span class="co"># RNN隐藏神经元个数</span></a><a class="sourceLine" id="cb3-21" data-line-number="21">            num_layers<span class="op">=</span><span class="dv">1</span>,  <span class="co"># RNN隐藏层个数</span></a><a class="sourceLine" id="cb3-22" data-line-number="22">        )</a><a class="sourceLine" id="cb3-23" data-line-number="23">        <span class="va">self</span>.out <span class="op">=</span> nn.Linear(<span class="dv">32</span>, <span class="dv">1</span>)</a><a class="sourceLine" id="cb3-24" data-line-number="24"></a><a class="sourceLine" id="cb3-25" data-line-number="25">    <span class="kw">def</span> forward(<span class="va">self</span>, x, h):</a><a class="sourceLine" id="cb3-26" data-line-number="26">        <span class="co"># x (time_step, batch_size, input_size)</span></a><a class="sourceLine" id="cb3-27" data-line-number="27">        <span class="co"># h (n_layers, batch, hidden_size)</span></a><a class="sourceLine" id="cb3-28" data-line-number="28">        <span class="co"># out (time_step, batch_size, hidden_size)</span></a><a class="sourceLine" id="cb3-29" data-line-number="29">        out, h <span class="op">=</span> <span class="va">self</span>.rnn(x, h)</a><a class="sourceLine" id="cb3-30" data-line-number="30">        prediction <span class="op">=</span> <span class="va">self</span>.out(out)</a><a class="sourceLine" id="cb3-31" data-line-number="31">        <span class="cf">return</span> prediction, h</a><a class="sourceLine" id="cb3-32" data-line-number="32"></a><a class="sourceLine" id="cb3-33" data-line-number="33"></a><a class="sourceLine" id="cb3-34" data-line-number="34">rnn <span class="op">=</span> RNN()</a><a class="sourceLine" id="cb3-35" data-line-number="35"><span class="bu">print</span>(rnn)</a><a class="sourceLine" id="cb3-36" data-line-number="36"></a><a class="sourceLine" id="cb3-37" data-line-number="37">optimizer <span class="op">=</span> torch.optim.Adam(rnn.parameters(), lr<span class="op">=</span>INIT_LR)</a><a class="sourceLine" id="cb3-38" data-line-number="38">loss_func <span class="op">=</span> nn.MSELoss()</a><a class="sourceLine" id="cb3-39" data-line-number="39">h_state <span class="op">=</span> <span class="va">None</span>  <span class="co"># 初始化隐藏层</span></a><a class="sourceLine" id="cb3-40" data-line-number="40"></a><a class="sourceLine" id="cb3-41" data-line-number="41">plt.figure()</a><a class="sourceLine" id="cb3-42" data-line-number="42">plt.ion()</a><a class="sourceLine" id="cb3-43" data-line-number="43"><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(N_EPOCHS):</a><a class="sourceLine" id="cb3-44" data-line-number="44">    start, end <span class="op">=</span> step <span class="op">*</span> np.pi, (step <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> np.pi  <span class="co"># 时间跨度</span></a><a class="sourceLine" id="cb3-45" data-line-number="45">    <span class="co"># 使用Sin函数预测Cos函数</span></a><a class="sourceLine" id="cb3-46" data-line-number="46">    steps <span class="op">=</span> np.linspace(start, end, TIME_STEP, dtype<span class="op">=</span>np.float32, endpoint<span class="op">=</span><span class="va">False</span>)</a><a class="sourceLine" id="cb3-47" data-line-number="47">    x_np <span class="op">=</span> np.sin(steps)</a><a class="sourceLine" id="cb3-48" data-line-number="48">    y_np <span class="op">=</span> np.cos(steps)</a><a class="sourceLine" id="cb3-49" data-line-number="49">    x <span class="op">=</span> torch.from_numpy(x_np[:, np.newaxis, np.newaxis])  <span class="co"># 尺寸大小为(time_step, batch, input_size)</span></a><a class="sourceLine" id="cb3-50" data-line-number="50">    y <span class="op">=</span> torch.from_numpy(y_np[:, np.newaxis, np.newaxis])</a><a class="sourceLine" id="cb3-51" data-line-number="51"></a><a class="sourceLine" id="cb3-52" data-line-number="52">    prediction, h_state <span class="op">=</span> rnn(x, h_state)  <span class="co"># RNN输出（预测结果，隐藏状态）</span></a><a class="sourceLine" id="cb3-53" data-line-number="53">    h_state <span class="op">=</span> h_state.detach()  <span class="co"># 这一行很重要，将每一次输出的中间状态传递下去(不带梯度)</span></a><a class="sourceLine" id="cb3-54" data-line-number="54">    loss <span class="op">=</span> loss_func(prediction, y)</a><a class="sourceLine" id="cb3-55" data-line-number="55">    optimizer.zero_grad()</a><a class="sourceLine" id="cb3-56" data-line-number="56">    loss.backward()</a><a class="sourceLine" id="cb3-57" data-line-number="57">    optimizer.step()</a><a class="sourceLine" id="cb3-58" data-line-number="58"></a><a class="sourceLine" id="cb3-59" data-line-number="59">    <span class="co"># 绘制中间结果</span></a><a class="sourceLine" id="cb3-60" data-line-number="60">    plt.cla()</a><a class="sourceLine" id="cb3-61" data-line-number="61">    plt.plot(steps, y_np, <span class="st">&#39;r-&#39;</span>)</a><a class="sourceLine" id="cb3-62" data-line-number="62">    plt.plot(steps, prediction.data.numpy().flatten(), <span class="st">&#39;b-&#39;</span>)</a><a class="sourceLine" id="cb3-63" data-line-number="63">    plt.draw()</a><a class="sourceLine" id="cb3-64" data-line-number="64">    plt.pause(<span class="fl">0.1</span>)</a><a class="sourceLine" id="cb3-65" data-line-number="65">plt.ioff()</a><a class="sourceLine" id="cb3-66" data-line-number="66">plt.show()</a></code></pre></div><p>最后的结果如下：</p><figure><img src="/images/ml/RNNSinCos.gif" alt="RNN使用Sin预测Cos"><figcaption>RNN使用Sin预测Cos</figcaption></figure><p>最后放一个当<code>TIME_STEP</code>分别等于10和20的最终预测结果的对比图：</p><figure><img src="/images/ml/Time-Step-10.png" alt="RNN TIME_STEP等于10"><figcaption>RNN TIME_STEP等于10</figcaption></figure><figure><img src="/images/ml/Time-Step-20.png" alt="RNN TIME_STEP=20"><figcaption>RNN TIME_STEP=20</figcaption></figure><p>第一张是<code>TIME_STEP</code>=10的预测结果，第二张是<code>TIME_STEP</code>=20的预测结果。为什么当<code>TIME_STEP</code>=20的预测结果差得十万八千里呢？</p><p>这是因为经典的RNN存在梯度爆炸和梯度弥散问题（我尝试修剪了梯度可是结果还是很差，不知道是不是其它原因），对长时序的预测表现很不好，所以才有了后来的LSTM和GRU等RNN变种。实际现在已经很少使用经典RNN了。有时间在说说LSTM吧，欢迎关注！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版权声明：本文为博主原创文章，转载请注明原文出处！ 写作时间：2019-03-02 12:46:15&lt;/p&gt;
&lt;p&gt;本文部分图片素材来自互联网，如有侵权，请联系作者删除！&lt;/p&gt;
&lt;h1 id=&quot;最简单的rnn回归模型入门pytorch版&quot;&gt;最简单的RNN回归模型入门（Py
      
    
    </summary>
    
      <category term="深度学习" scheme="http://theonegis.github.io/categories/dl/"/>
    
    
      <category term="PyTorch" scheme="http://theonegis.github.io/tags/PyTorch/"/>
    
      <category term="RNN" scheme="http://theonegis.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode-Longest Palindromic Subsequence</title>
    <link href="http://theonegis.github.io/algorithm/LeetCode-Longest-Palindromic-Subsequence/"/>
    <id>http://theonegis.github.io/algorithm/LeetCode-Longest-Palindromic-Subsequence/</id>
    <published>2019-02-10T03:44:52.000Z</published>
    <updated>2019-03-22T19:39:53.823Z</updated>
    
    <content type="html"><![CDATA[<p>版权声明：本文为博主原创文章，转载请注明原文出处！ 写作时间：2019-02-10 11:44:52</p><h1 id="longest-palindromic-subsequence">Longest Palindromic Subsequence</h1><h2 id="题目描述">题目描述</h2><p>这是LeetCode的第516道题目：<a href="https://leetcode.com/problems/longest-palindromic-subsequence/" target="_blank" rel="noopener">516. Longest Palindromic Subsequence</a>。</p><p>Given a string s, find the longest palindromic subsequence’s length in s. You may assume that the maximum length of s is 1000.</p><p><strong>Example 1:</strong> Input:</p><pre><code>&quot;bbbab&quot;</code></pre><p>Output:</p><pre><code>4</code></pre><p>One possible longest palindromic subsequence is “bbbb”.</p><p><strong>Example 2:</strong> Input:</p><pre><code>&quot;cbbd&quot;</code></pre><p>Output:</p><pre><code>2</code></pre><p>One possible longest palindromic subsequence is “bb”.</p><p>题目要求我们计算出给定字符串中的最长回文序列（这里的序列不是一定要在给定字符串中连续排列的，就是挑出的单个字符按其在给定字符串中的顺序排列以后是回文即可）</p><h2 id="思路分析">思路分析</h2><p>其实，思路跟第647道题目<a href="https://leetcode.com/problems/palindromic-substrings/" target="_blank" rel="noopener">Palindromic Substrings</a>是类似的，可以采用动态规划进行。但是因为回文序列不是给定字符串的连续子串，貌似不能使用中心扩散法求解。</p><p>使用动态规划，我们设<code>dp[i][j]</code>表示从第<code>i</code>个字符到到<code>j</code>个字符回文序列的长度，则有：</p><ol type="1"><li>当<code>s[i] == s[j]</code>时，<code>dp[i][j] = dp[i+1][j-1] + 2</code></li><li>当<code>s[i] != s[j]</code>时，<code>dp[i][j] = max(dp[i+1][j], dp[i][j-1])</code></li></ol><h2 id="c实现">C++实现</h2><div class="sourceCode" id="cb5"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">class</span> Solution {</a><a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">public</span>:</a><a class="sourceLine" id="cb5-3" data-line-number="3">    <span class="dt">int</span> longestPalindromeSubseq(string s) {</a><a class="sourceLine" id="cb5-4" data-line-number="4">        <span class="at">const</span> <span class="dt">int</span> length = s.length();</a><a class="sourceLine" id="cb5-5" data-line-number="5">        vector&lt;vector&lt;<span class="dt">int</span>&gt;&gt; dp(length, vector&lt;<span class="dt">int</span>&gt;(length));</a><a class="sourceLine" id="cb5-6" data-line-number="6">        <span class="cf">for</span> (<span class="kw">auto</span> i = length - <span class="dv">1</span>; i &gt;= <span class="dv">0</span>; --i) {</a><a class="sourceLine" id="cb5-7" data-line-number="7">            dp[i][i] = <span class="dv">1</span>;</a><a class="sourceLine" id="cb5-8" data-line-number="8">            <span class="cf">for</span> (<span class="kw">auto</span> j = i + <span class="dv">1</span>; j &lt; length; ++j) {</a><a class="sourceLine" id="cb5-9" data-line-number="9">                dp[i][j] = s[i] == s[j] ?</a><a class="sourceLine" id="cb5-10" data-line-number="10">                        dp[i + <span class="dv">1</span>][j - <span class="dv">1</span>] + <span class="dv">2</span> :</a><a class="sourceLine" id="cb5-11" data-line-number="11">                        max(dp[i + <span class="dv">1</span>][j], dp[i][j - <span class="dv">1</span>]);</a><a class="sourceLine" id="cb5-12" data-line-number="12">            }</a><a class="sourceLine" id="cb5-13" data-line-number="13">        }</a><a class="sourceLine" id="cb5-14" data-line-number="14">        <span class="cf">return</span> dp[<span class="dv">0</span>][length - <span class="dv">1</span>];</a><a class="sourceLine" id="cb5-15" data-line-number="15">    }</a><a class="sourceLine" id="cb5-16" data-line-number="16">};</a></code></pre></div><h2 id="scala实现">Scala实现</h2><p>Scala版本是对C++版本的翻译</p><div class="sourceCode" id="cb6"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="kw">object</span> Solution {</a><a class="sourceLine" id="cb6-2" data-line-number="2">  <span class="kw">def</span> <span class="fu">longestPalindromeSubseq</span>(s: String): Int = {</a><a class="sourceLine" id="cb6-3" data-line-number="3">    <span class="kw">val</span> length = s.<span class="fu">length</span></a><a class="sourceLine" id="cb6-4" data-line-number="4">    <span class="kw">val</span> dp = Array.<span class="fu">ofDim</span>[Int](length, length)</a><a class="sourceLine" id="cb6-5" data-line-number="5">    <span class="kw">for</span> (i &lt;- length - <span class="dv">1</span> to <span class="dv">0</span> by <span class="dv">-1</span>) {</a><a class="sourceLine" id="cb6-6" data-line-number="6">      <span class="fu">dp</span>(i)(i) = <span class="dv">1</span></a><a class="sourceLine" id="cb6-7" data-line-number="7">      <span class="kw">for</span> (j &lt;- i + <span class="dv">1</span> until length) {</a><a class="sourceLine" id="cb6-8" data-line-number="8">        <span class="fu">dp</span>(i)(j) = <span class="kw">if</span> (<span class="fu">s</span>(i) == <span class="fu">s</span>(j)) <span class="fu">dp</span>(i + <span class="dv">1</span>)(j - <span class="dv">1</span>) + <span class="dv">2</span></a><a class="sourceLine" id="cb6-9" data-line-number="9">        <span class="kw">else</span> math.<span class="fu">max</span>(<span class="fu">dp</span>(i + <span class="dv">1</span>)(j), <span class="fu">dp</span>(i)(j - <span class="dv">1</span>))</a><a class="sourceLine" id="cb6-10" data-line-number="10">      }</a><a class="sourceLine" id="cb6-11" data-line-number="11">    }</a><a class="sourceLine" id="cb6-12" data-line-number="12">    <span class="kw">return</span> <span class="fu">dp</span>(<span class="dv">0</span>)(length - <span class="dv">1</span>)</a><a class="sourceLine" id="cb6-13" data-line-number="13">  }</a><a class="sourceLine" id="cb6-14" data-line-number="14">}</a></code></pre></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版权声明：本文为博主原创文章，转载请注明原文出处！ 写作时间：2019-02-10 11:44:52&lt;/p&gt;
&lt;h1 id=&quot;longest-palindromic-subsequence&quot;&gt;Longest Palindromic Subsequence&lt;/h1&gt;
&lt;h2 
      
    
    </summary>
    
      <category term="算法" scheme="http://theonegis.github.io/categories/algorithm/"/>
    
    
      <category term="LeetCode" scheme="http://theonegis.github.io/tags/LeetCode/"/>
    
      <category term="动态规划" scheme="http://theonegis.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
      <category term="回文" scheme="http://theonegis.github.io/tags/%E5%9B%9E%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Longest Palindromic Substring</title>
    <link href="http://theonegis.github.io/algorithm/Longest-Palindromic-Substring/"/>
    <id>http://theonegis.github.io/algorithm/Longest-Palindromic-Substring/</id>
    <published>2019-02-09T16:04:34.000Z</published>
    <updated>2019-03-22T19:39:53.824Z</updated>
    
    <content type="html"><![CDATA[<p>版权声明：本文为博主原创文章，转载请注明原文出处！</p><p>写作时间：2019-02-10 00:04:34</p><h1 id="leetcode-longest-palindromic-substring">LeetCode-Longest Palindromic Substring</h1><h2 id="题目描述">题目描述</h2><p>LeetCode第5道题目：<a href="https://leetcode.com/problems/longest-palindromic-substring/" target="_blank" rel="noopener">5. Longest Palindromic Substring</a></p><p>Given a string <strong>s</strong>, find the longest palindromic substring in <strong>s</strong>. You may assume that the maximum length of <strong>s</strong> is 1000.</p><p><strong>Example 1:</strong></p><pre><code>Input: &quot;babad&quot;Output: &quot;bab&quot;Note: &quot;aba&quot; is also a valid answer.</code></pre><p><strong>Example 2:</strong></p><pre><code>Input: &quot;cbbd&quot;Output: &quot;bb&quot;</code></pre><p>题目要求我们找到给定字符串中的所有回文子串中最长子串。</p><h2 id="思路分析">思路分析</h2><p>这个题目和之前的<a href="https://theonegis.github.io/algorithm/LeetCode-Palindromic-Substrings/">LeetCode-Palindromic Substrings</a>题目的思路是一样的，<a href="https://leetcode.com/problems/palindromic-substrings/" target="_blank" rel="noopener">Palindromic Substrings</a>是找回文的个数。在这个过程中其实我们是找出了所有的回文子串，接下来我们统计一下每个回文的长度，选择出最长的那个就是本文的答案了（当然，在代码实现过程中统计最长子串不一定是找到了所有的子串之后再统计，可能是边寻找边统计）。所以，方法还是老方法，可以利用动态规划，也可以利用中心扩散法。</p><p>有不明白的地方可以参见我之前的博文《<a href="https://theonegis.github.io/algorithm/LeetCode-Palindromic-Substrings/">LeetCode-Palindromic Substrings</a>》，这里我只给出了使用中心扩散法进行求解的代码实现。</p><h2 id="c实现">C++实现</h2><div class="sourceCode" id="cb3"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">class</span> Solution {</a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="kw">private</span>:</a><a class="sourceLine" id="cb3-3" data-line-number="3">    <span class="dt">int</span> longest = <span class="dv">0</span>;            <span class="co">// 记录最长子串的字符个数</span></a><a class="sourceLine" id="cb3-4" data-line-number="4">    string palindrome = <span class="st">&quot;&quot;</span>;     <span class="co">// 保存最长的子串</span></a><a class="sourceLine" id="cb3-5" data-line-number="5">    <span class="dt">void</span> extendPalindrome(<span class="at">const</span> string&amp; s, <span class="dt">int</span> left, <span class="dt">int</span> right) {</a><a class="sourceLine" id="cb3-6" data-line-number="6">        <span class="cf">while</span> ((left &gt;= <span class="dv">0</span>) &amp;&amp; (right &lt; s.size()) &amp;&amp; (s[left] == s[right])) {</a><a class="sourceLine" id="cb3-7" data-line-number="7">            --left;</a><a class="sourceLine" id="cb3-8" data-line-number="8">            ++right;</a><a class="sourceLine" id="cb3-9" data-line-number="9">        }</a><a class="sourceLine" id="cb3-10" data-line-number="10">        <span class="dt">int</span> count = right - left - <span class="dv">1</span>;</a><a class="sourceLine" id="cb3-11" data-line-number="11">        <span class="cf">if</span> (count &gt; longest) {</a><a class="sourceLine" id="cb3-12" data-line-number="12">            longest = count;</a><a class="sourceLine" id="cb3-13" data-line-number="13">            palindrome = s.substr(left + <span class="dv">1</span>, longest);</a><a class="sourceLine" id="cb3-14" data-line-number="14">        }</a><a class="sourceLine" id="cb3-15" data-line-number="15">    }</a><a class="sourceLine" id="cb3-16" data-line-number="16"></a><a class="sourceLine" id="cb3-17" data-line-number="17"><span class="kw">public</span>:</a><a class="sourceLine" id="cb3-18" data-line-number="18">    string longestPalindrome(string s) {</a><a class="sourceLine" id="cb3-19" data-line-number="19">        <span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; s.size(); ++i) {</a><a class="sourceLine" id="cb3-20" data-line-number="20">            extendPalindrome(s, i, i);</a><a class="sourceLine" id="cb3-21" data-line-number="21">            extendPalindrome(s, i, i + <span class="dv">1</span>);</a><a class="sourceLine" id="cb3-22" data-line-number="22">        }</a><a class="sourceLine" id="cb3-23" data-line-number="23">        <span class="cf">return</span> palindrome;</a><a class="sourceLine" id="cb3-24" data-line-number="24">    }</a><a class="sourceLine" id="cb3-25" data-line-number="25">};</a></code></pre></div><p>可以对比一下，和<a href="https://theonegis.github.io/algorithm/LeetCode-Palindromic-Substrings/">LeetCode-Palindromic Substrings</a>的答案是不是没多大变化？</p><h2 id="scala实现">Scala实现</h2><div class="sourceCode" id="cb4"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">object</span> Solution {</a><a class="sourceLine" id="cb4-2" data-line-number="2">  <span class="kw">def</span> <span class="fu">longestPalindrome</span>(s: String): String = {</a><a class="sourceLine" id="cb4-3" data-line-number="3">    s.<span class="fu">length</span> <span class="kw">match</span> {</a><a class="sourceLine" id="cb4-4" data-line-number="4">      <span class="kw">case</span> <span class="dv">0</span> =&gt; s</a><a class="sourceLine" id="cb4-5" data-line-number="5">      <span class="kw">case</span> _ =&gt; {</a><a class="sourceLine" id="cb4-6" data-line-number="6">        <span class="kw">val</span> longest = (<span class="kw">for</span> {</a><a class="sourceLine" id="cb4-7" data-line-number="7">          i &lt;- s.<span class="fu">indices</span></a><a class="sourceLine" id="cb4-8" data-line-number="8">          j &lt;- List(i, i + <span class="dv">1</span>)</a><a class="sourceLine" id="cb4-9" data-line-number="9">          k &lt;- (i to <span class="dv">0</span> by <span class="dv">-1</span>).<span class="fu">zip</span>(j until s.<span class="fu">length</span>).<span class="fu">takeWhile</span>(p =&gt; <span class="fu">s</span>(p._<span class="dv">1</span>) == <span class="fu">s</span>(p._<span class="dv">2</span>))</a><a class="sourceLine" id="cb4-10" data-line-number="10">        } <span class="kw">yield</span> (k._<span class="dv">1</span>, k._<span class="dv">2</span>)).<span class="fu">maxBy</span>(p =&gt; p._<span class="dv">2</span> - p._<span class="dv">1</span>)</a><a class="sourceLine" id="cb4-11" data-line-number="11">        s.<span class="fu">substring</span>(longest._<span class="dv">1</span>, longest._<span class="dv">2</span> + <span class="dv">1</span>)</a><a class="sourceLine" id="cb4-12" data-line-number="12">      }</a><a class="sourceLine" id="cb4-13" data-line-number="13">    }</a><a class="sourceLine" id="cb4-14" data-line-number="14">  }</a><a class="sourceLine" id="cb4-15" data-line-number="15">}</a></code></pre></div><p>这里需要注意的是：</p><ol type="1"><li><p>需要对于空字符串的特殊处理（这里使用的是<code>match</code>匹配，当然也可以使用<code>if...else</code>条件语句）</p></li><li>我们使用<code>yield</code>每次生成回文子串的左右指针，然后再使用<code>maxBy</code>得到最长子串对应的左右指针。</li><li><p><code>takeWhile</code>函数对集合进行遍历过程中当条件不满足的时候会立即停止判断，返回的是最后那个满足的元素。（<code>filter</code>函数会返回集合中所有满足条件的元素）</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;版权声明：本文为博主原创文章，转载请注明原文出处！&lt;/p&gt;
&lt;p&gt;写作时间：2019-02-10 00:04:34&lt;/p&gt;
&lt;h1 id=&quot;leetcode-longest-palindromic-substring&quot;&gt;LeetCode-Longest Palindromi
      
    
    </summary>
    
      <category term="算法" scheme="http://theonegis.github.io/categories/algorithm/"/>
    
    
      <category term="LeetCode" scheme="http://theonegis.github.io/tags/LeetCode/"/>
    
      <category term="回文" scheme="http://theonegis.github.io/tags/%E5%9B%9E%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode-Palindromic Substrings</title>
    <link href="http://theonegis.github.io/algorithm/LeetCode-Palindromic-Substrings/"/>
    <id>http://theonegis.github.io/algorithm/LeetCode-Palindromic-Substrings/</id>
    <published>2019-02-07T02:54:47.000Z</published>
    <updated>2019-03-22T19:39:53.823Z</updated>
    
    <content type="html"><![CDATA[<h1 id="leetcode-palindromic-substrings">LeetCode-Palindromic Substrings</h1><h2 id="题目描述">题目描述</h2><p>这是第647道题目：<a href="https://leetcode.com/problems/palindromic-substrings/" target="_blank" rel="noopener">Palindromic Substrings</a></p><p>Given a string, your task is to count how many palindromic substrings in this string.</p><p>The substrings with different start indexes or end indexes are counted as different substrings even they consist of same characters.</p><p><strong>Example 1:</strong></p><pre><code>Input: &quot;abc&quot;Output: 3Explanation: Three palindromic strings: &quot;a&quot;, &quot;b&quot;, &quot;c&quot;.</code></pre><p><strong>Example 2:</strong></p><pre><code>Input: &quot;aaa&quot;Output: 6Explanation: Six palindromic strings: &quot;a&quot;, &quot;a&quot;, &quot;a&quot;, &quot;aa&quot;, &quot;aa&quot;, &quot;aaa&quot;.</code></pre><p>题目要求是需要计算出给定字符串中所有回文子串的个数（单个字符也算一个回文子串，不同索引位置的相同内容的回文子串也算不同的回文）</p><h2 id="思路分析">思路分析</h2><p>有两种思路：一种是采用动态规划的方法；另一种是采用中心扩散的方法。</p><ol type="1"><li><p>动态规划</p><p>如果用<code>dp[i][j]</code>表示从第<code>i</code>个字符到第<code>j</code>个字符是不是回文子串，<code>s</code>表示给定字符串，则有</p><p><code>dp[i][j]</code>= (<code>s[i]</code> == <code>s[j]</code>) &amp;&amp; (<code>i</code> - <code>j</code> &lt; 2) （这里表示的是子串是一个字符，两个字符的情形）</p><p><code>dp[i][j]</code>= (<code>s[i]</code> == <code>s[j]</code>) &amp;&amp; (<code>dp[i + 1][j -1]</code> ) （这里表示的是除了前面两种情形之外的情形）</p><p>注：三个字符串的情形既可以归类到第一种情况（如果归类到第一种情况，则条件需要变为<code>i</code> - <code>j</code> &lt; 3），也可以归类到第二种情形</p></li><li><p>中心扩散</p><p>扩散法假定一个中心，然后采用左右两个指针同时向两边走来判断是不是回文。</p><p>注：中心扩散法需要区分回文子串中的字符个数是奇数和偶数两种情况。</p></li></ol><h2 id="c实现">C++实现</h2><ol type="1"><li><p>动态规划</p><div class="sourceCode" id="cb3"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">class</span> Solution {</a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="kw">public</span>:</a><a class="sourceLine" id="cb3-3" data-line-number="3">    <span class="dt">int</span> countSubstrings(string s) {</a><a class="sourceLine" id="cb3-4" data-line-number="4">        <span class="at">const</span> <span class="dt">int</span> N = <span class="kw">static_cast</span>&lt;<span class="dt">int</span>&gt;(s.size());   <span class="co">// 如果不强转就会超时，好奇怪</span></a><a class="sourceLine" id="cb3-5" data-line-number="5">        <span class="dt">int</span> count = <span class="dv">0</span>;</a><a class="sourceLine" id="cb3-6" data-line-number="6">        <span class="co">// 下面这一行换成原生数组也是可以的int dp[N][N]</span></a><a class="sourceLine" id="cb3-7" data-line-number="7">        vector&lt;vector&lt;<span class="dt">bool</span>&gt;&gt; dp(N, vector&lt;<span class="dt">bool</span>&gt;(N));</a><a class="sourceLine" id="cb3-8" data-line-number="8">        <span class="co">// 从后面遍历是为了让求dp[i][j]的时候dp[i + 1][j - 1]是已经计算过的</span></a><a class="sourceLine" id="cb3-9" data-line-number="9">        <span class="cf">for</span> (<span class="kw">auto</span> i = N - <span class="dv">1</span>; i &gt;= <span class="dv">0</span>; --i) {</a><a class="sourceLine" id="cb3-10" data-line-number="10">            <span class="cf">for</span> (<span class="kw">auto</span> j = i; j &lt; N; ++j) {</a><a class="sourceLine" id="cb3-11" data-line-number="11">                <span class="co">// (j - i &lt; 2)包含了两种情况，使得dp[i + 1][j - 1]可以包含剩下的所有情况</span></a><a class="sourceLine" id="cb3-12" data-line-number="12">                dp[i][j] = (s[i] == s[j]) &amp;&amp; ((j - i &lt; <span class="dv">2</span>) || dp[i + <span class="dv">1</span>][j - <span class="dv">1</span>]);</a><a class="sourceLine" id="cb3-13" data-line-number="13">                <span class="cf">if</span> (dp[i][j]) count++;</a><a class="sourceLine" id="cb3-14" data-line-number="14">            }</a><a class="sourceLine" id="cb3-15" data-line-number="15">        }</a><a class="sourceLine" id="cb3-16" data-line-number="16"></a><a class="sourceLine" id="cb3-17" data-line-number="17">        <span class="cf">return</span> count;</a><a class="sourceLine" id="cb3-18" data-line-number="18">    }</a><a class="sourceLine" id="cb3-19" data-line-number="19">};</a></code></pre></div><p>在使用C++实现的时候，我发现一些有意思的现象：</p><ol type="1"><li>在第四行<code>s.size()</code>的返回类型本来是<code>size_t</code>，但是如果直接使用<code>size_t</code>的话，运行直接超时。我强制转换为<code>int</code>以后就可以通过测试。有童鞋能帮我解答一下疑惑吗？🙏</li><li>用于存储<code>dp</code>的使用动态数组<code>vector</code>是一般都会想到的，但是我看到一些提交中也有直接使用C++原生数组的。我就奇怪了，C++原生数组的话需要使用<code>new</code>操作符去动态申请，为什么直接使用也可以通过编译呢？我后来查了一些资料，原来C99标准中支持了原生动态数组（标准中称之为变成数组variable length array）。但是C++标准中这个特性是可选的，就是说可能有的编译器支持这样写，而有的编译器不行。不过，原生数组相对<code>vector</code>容器，效率会更高一些。如果你的编译器支持，大胆地使用吧！</li></ol></li><li><p>中心扩散</p><div class="sourceCode" id="cb4"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">class</span> Solution {</a><a class="sourceLine" id="cb4-2" data-line-number="2"><span class="kw">private</span>:</a><a class="sourceLine" id="cb4-3" data-line-number="3">    <span class="dt">int</span> count = <span class="dv">0</span>;</a><a class="sourceLine" id="cb4-4" data-line-number="4">    <span class="dt">void</span> extendPalindrome(<span class="at">const</span> string&amp; s, <span class="dt">int</span> left, <span class="dt">int</span> right) {</a><a class="sourceLine" id="cb4-5" data-line-number="5">        <span class="cf">while</span> ((left &gt;= <span class="dv">0</span>) &amp;&amp; (right &lt; s.size()) &amp;&amp; (s[left] == s[right])) {</a><a class="sourceLine" id="cb4-6" data-line-number="6">            --left;</a><a class="sourceLine" id="cb4-7" data-line-number="7">            ++right;</a><a class="sourceLine" id="cb4-8" data-line-number="8">            ++count;</a><a class="sourceLine" id="cb4-9" data-line-number="9">        }</a><a class="sourceLine" id="cb4-10" data-line-number="10">    }</a><a class="sourceLine" id="cb4-11" data-line-number="11"></a><a class="sourceLine" id="cb4-12" data-line-number="12"><span class="kw">public</span>:</a><a class="sourceLine" id="cb4-13" data-line-number="13">    <span class="dt">int</span> countSubstrings(string s) {</a><a class="sourceLine" id="cb4-14" data-line-number="14">        <span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">0</span>; i &lt; s.size(); ++i) {</a><a class="sourceLine" id="cb4-15" data-line-number="15">            extendPalindrome(s, i, i);      <span class="co">// 对于奇数个字符的回文</span></a><a class="sourceLine" id="cb4-16" data-line-number="16">            extendPalindrome(s, i, i + <span class="dv">1</span>);  <span class="co">// 对于偶数个字符的回文</span></a><a class="sourceLine" id="cb4-17" data-line-number="17">        }</a><a class="sourceLine" id="cb4-18" data-line-number="18">        <span class="cf">return</span> count;</a><a class="sourceLine" id="cb4-19" data-line-number="19">    }</a><a class="sourceLine" id="cb4-20" data-line-number="20">};</a></code></pre></div></li></ol><h2 id="scala实现">Scala实现</h2><p>Scala的实现是在LeetCode上看到一个大神的答案，使用纯函数实现，写得很美妙，拿过来与大家分享！</p><div class="sourceCode" id="cb5"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">object</span> Solution {</a><a class="sourceLine" id="cb5-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">countSubstrings</span>(s: String): Int = {</a><a class="sourceLine" id="cb5-3" data-line-number="3">        (<span class="kw">for</span> {</a><a class="sourceLine" id="cb5-4" data-line-number="4">            i &lt;- <span class="dv">0</span> until s.<span class="fu">length</span></a><a class="sourceLine" id="cb5-5" data-line-number="5">            j &lt;- List(i, i + <span class="dv">1</span>)</a><a class="sourceLine" id="cb5-6" data-line-number="6">            _ &lt;- (i to <span class="dv">0</span> by <span class="dv">-1</span>).<span class="fu">zip</span>(j until s.<span class="fu">length</span>).<span class="fu">takeWhile</span>(p =&gt; <span class="fu">s</span>(p._<span class="dv">1</span>) == <span class="fu">s</span>(p._<span class="dv">2</span>))</a><a class="sourceLine" id="cb5-7" data-line-number="7">        } <span class="kw">yield</span> <span class="kw">true</span>).<span class="fu">length</span></a><a class="sourceLine" id="cb5-8" data-line-number="8">    }</a><a class="sourceLine" id="cb5-9" data-line-number="9">}</a></code></pre></div><p>这也是采用中心扩散法实现的。</p><p><code>for</code>循环中的<code>i</code>从左到右依次遍历给定字符串，<code>j</code>控制的是奇数个数的子串情况和偶数个数的子串情况，<code>for</code>循环中的第三个匿名变量其实相当于一个条件判断。整个<code>for</code>循环返回一个<code>vector</code>（里面都是<code>true</code>），最后统计这个<code>vector</code>个中包含元素的个数即可。</p><p>这里重点说一下<code>for</code>循环中的第三个匿名循环控制语句。<code>(i to 0 by -1).zip(j until s.length)</code>生成一个从中间向两边扩散的<code>List</code>（其实是<code>List</code>的子类<code>::</code>非空链表），这个<code>List</code>中的每个元素是一个<code>Tuple2</code>包含的是左指针<code>i</code>和右指针<code>j</code>。<code>takeWhile</code>方法是起到一个过滤作用，将左指针和右指针指向的值相等的这<code>Tuple2</code>返回（其实返回类型是<code>::</code>,只是里面只有一个元素）。如果左指针和右指针指向的值不相等，则返回<code>Nil（一个空的List）</code>。如果返回的是<code>Nil</code>的话，则不会生成一个<code>true</code>。这样子，其实第三个循环控制语句起到的是判断的作用。</p><p>注：</p><ol type="1"><li>Scala中的<code>Vector</code>类似于Java中的<code>ArrayList</code>，而Scala中的<code>List</code>类似于Java中的<code>LinkedList</code></li><li>Scala中的<code>List</code>有两个特殊的子类：<code>::</code>表示非空的<code>List</code>，<code>Nil</code>表示空的<code>List</code></li><li>函数<code>filter</code>和<code>takeWhile</code>都可以起到过滤的作用，<code>filter</code>会过滤出给定集合中所有满足条件的元素，而<code>takeWhile</code>只会返回第一个满足条件的元素。但是两者返回的都是集合，即使<code>takeWhile</code>返回的集合只有一个元素。</li></ol><p>感觉函数式编程是挺好玩的，只是现在水平有限，还玩不起来！继续加油！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;leetcode-palindromic-substrings&quot;&gt;LeetCode-Palindromic Substrings&lt;/h1&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;题目描述&lt;/h2&gt;
&lt;p&gt;这是第647道题目：&lt;a href=&quot;https://leetcode
      
    
    </summary>
    
      <category term="算法" scheme="http://theonegis.github.io/categories/algorithm/"/>
    
    
      <category term="LeetCode" scheme="http://theonegis.github.io/tags/LeetCode/"/>
    
      <category term="动态规划" scheme="http://theonegis.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
      <category term="回文" scheme="http://theonegis.github.io/tags/%E5%9B%9E%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode-Minimum Falling Path Sum</title>
    <link href="http://theonegis.github.io/algorithm/LeetCode-Minimum-Falling-Path-Sum/"/>
    <id>http://theonegis.github.io/algorithm/LeetCode-Minimum-Falling-Path-Sum/</id>
    <published>2019-02-04T13:13:56.000Z</published>
    <updated>2019-03-22T19:39:53.823Z</updated>
    
    <content type="html"><![CDATA[<h1 id="minimum-falling-path-sum">Minimum Falling Path Sum</h1><h2 id="题目描述">题目描述</h2><p>本题目链接：<a href="https://leetcode.com/problems/minimum-falling-path-sum/" target="_blank" rel="noopener">931. Minimum Falling Path Sum</a></p><p>Given a <strong>square</strong> array of integers <code>A</code>, we want the <strong>minimum</strong> sum of a <em>falling path</em>through <code>A</code>.</p><p>A falling path starts at any element in the first row, and chooses one element from each row. The next row’s choice must be in a column that is different from the previous row’s column by at most one.</p><p>题目的意思是在一个给定的二维方格中，从上往下走。列方向每次只走一步，行方向上最多只能跨越一个单元格。即就是只能向正下方，左下方，右下方行进。每个方格都有一个值，目标是走到最后一行的路径中包含的值之和最小。</p><h2 id="问题分析">问题分析</h2><p>还是使用动态规划，而动态规划的重中之重就是建立递推关系。</p><p>显然，对于第一行，我们选择最小的数进行开始；</p><p>然后，对于后面的，我们每次只要选择正下方，左下方，右下方中最小的数即可。</p><p>递推公式为：<code>dp[i][j] = dp[i-1][j] + min(A[i][j-1], A[i][j], A[i][j+1])</code>（注意对数组越界的处理）</p><h2 id="c实现">C++实现</h2><p>使用<code>A</code>当做<code>dp</code>数组，这样可以节省空间，但是我觉得对输入参数直接进行了修改，这样不是很好。</p><div class="sourceCode" id="cb1"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">class</span> Solution {</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">public</span>:</a><a class="sourceLine" id="cb1-3" data-line-number="3">    <span class="dt">int</span> minFallingPathSum(vector&lt;vector&lt;<span class="dt">int</span>&gt;&gt; &amp;A) {</a><a class="sourceLine" id="cb1-4" data-line-number="4">        <span class="cf">for</span> (<span class="kw">auto</span> i = <span class="dv">1</span>; i &lt; A.size(); ++i) {</a><a class="sourceLine" id="cb1-5" data-line-number="5">            <span class="cf">for</span> (<span class="kw">auto</span> j = <span class="dv">0</span>; j &lt; A.size(); ++j) {</a><a class="sourceLine" id="cb1-6" data-line-number="6">                A[i][j] +=</a><a class="sourceLine" id="cb1-7" data-line-number="7">                        min({A[i - <span class="dv">1</span>][max(<span class="dv">0</span>, j - <span class="dv">1</span>)],</a><a class="sourceLine" id="cb1-8" data-line-number="8">                             A[i - <span class="dv">1</span>][j],</a><a class="sourceLine" id="cb1-9" data-line-number="9">                             A[i - <span class="dv">1</span>][min(<span class="kw">static_cast</span>&lt;<span class="dt">int</span>&gt;(A.size() - <span class="dv">1</span>), j + <span class="dv">1</span>)]});</a><a class="sourceLine" id="cb1-10" data-line-number="10">            }</a><a class="sourceLine" id="cb1-11" data-line-number="11"></a><a class="sourceLine" id="cb1-12" data-line-number="12">        }</a><a class="sourceLine" id="cb1-13" data-line-number="13">        <span class="cf">return</span> *min_element(A.back().begin(), A.back().end());</a><a class="sourceLine" id="cb1-14" data-line-number="14">    }</a><a class="sourceLine" id="cb1-15" data-line-number="15">};</a></code></pre></div><h2 id="scala实现">Scala实现</h2><p>Scala版本的对输入参数<code>A</code>保持不变，但是这仍然不是纯函数的实现。如果有朋友有纯函数实现的方案，请不吝赐教！</p><div class="sourceCode" id="cb2"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">object</span> Solution {</a><a class="sourceLine" id="cb2-2" data-line-number="2">  <span class="kw">def</span> <span class="fu">minFallingPathSum</span>(A: Array[Array[Int]]): Int = {</a><a class="sourceLine" id="cb2-3" data-line-number="3">    <span class="kw">val</span> dp = A.<span class="fu">clone</span>()</a><a class="sourceLine" id="cb2-4" data-line-number="4">    <span class="kw">for</span> (i &lt;- <span class="dv">1</span> until dp.<span class="fu">length</span>; j &lt;- dp.<span class="fu">indices</span>) {</a><a class="sourceLine" id="cb2-5" data-line-number="5">      <span class="fu">dp</span>(i)(j) += List(</a><a class="sourceLine" id="cb2-6" data-line-number="6">        <span class="fu">dp</span>(i - <span class="dv">1</span>)(math.<span class="fu">max</span>(<span class="dv">0</span>, j - <span class="dv">1</span>)),</a><a class="sourceLine" id="cb2-7" data-line-number="7">        <span class="fu">dp</span>(i - <span class="dv">1</span>)(j),</a><a class="sourceLine" id="cb2-8" data-line-number="8">        <span class="fu">dp</span>(i - <span class="dv">1</span>)(math.<span class="fu">min</span>(dp.<span class="fu">length</span> - <span class="dv">1</span>, j + <span class="dv">1</span>))).<span class="fu">min</span></a><a class="sourceLine" id="cb2-9" data-line-number="9">    }</a><a class="sourceLine" id="cb2-10" data-line-number="10">    dp.<span class="fu">last</span>.<span class="fu">min</span></a><a class="sourceLine" id="cb2-11" data-line-number="11">  }</a><a class="sourceLine" id="cb2-12" data-line-number="12">}</a></code></pre></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;minimum-falling-path-sum&quot;&gt;Minimum Falling Path Sum&lt;/h1&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;题目描述&lt;/h2&gt;
&lt;p&gt;本题目链接：&lt;a href=&quot;https://leetcode.com/problems/mini
      
    
    </summary>
    
      <category term="算法" scheme="http://theonegis.github.io/categories/algorithm/"/>
    
    
      <category term="LeetCode" scheme="http://theonegis.github.io/tags/LeetCode/"/>
    
      <category term="动态规划" scheme="http://theonegis.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode-Minimum Cost For Tickets</title>
    <link href="http://theonegis.github.io/algorithm/LeetCode-Minimum-Cost-For-Tickets/"/>
    <id>http://theonegis.github.io/algorithm/LeetCode-Minimum-Cost-For-Tickets/</id>
    <published>2019-02-04T04:41:11.000Z</published>
    <updated>2019-03-22T19:39:53.823Z</updated>
    
    <content type="html"><![CDATA[<h1 id="minimum-cost-for-tickets">Minimum Cost For Tickets</h1><h2 id="题目描述">题目描述</h2><p>LeetCode地址：<a href="https://leetcode.com/problems/minimum-cost-for-tickets/" target="_blank" rel="noopener">983. Minimum Cost For Tickets</a></p><p>In a country popular for train travel, you have planned some train travelling one year in advance. The days of the year that you will travel is given as an array <code>days</code>. Each day is an integer from <code>1</code> to <code>365</code>.</p><p>Train tickets are sold in 3 different ways:</p><ul><li>a 1-day pass is sold for <code>costs[0]</code> dollars;</li><li>a 7-day pass is sold for <code>costs[1]</code> dollars;</li><li>a 30-day pass is sold for <code>costs[2]</code> dollars.</li></ul><p>The passes allow that many days of consecutive travel. For example, if we get a 7-day pass on day 2, then we can travel for 7 days: day 2, 3, 4, 5, 6, 7, and 8.</p><p>Return the minimum number of dollars you need to travel every day in the given list of <code>days</code>.</p><p>days数组中存储的是该年中去旅游的日期（范围为1到365之间的数字），costs数组大小为3，存储的是1天，7天和30天火车票的价格。我们需要做一个方案选择合适的购票方案达到旅游days天最省钱的目的。</p><h2 id="算法描述">算法描述</h2><p>采用动态规划进行解决，假设现在是第days[i]天，我们在该天出行旅游需要选择买票方案，现在我们有三种方案：第一，购买一天的通行票，当天出行，花费就是第days[i-1]天的花费加上一天的通行票价；第二，购买七天的通行票，而七天的通行票可以在连续的七天之内使用，所以花费是第days[i-7]天的花费加上七天的通行票价（即从第days[i-8]天到days[i]天的花费都包含在这七天的通行票中）；第三，购买三十天的通行票，同理，花费是days[i-30]天加上三十天的通行票价。然后我们在这三种方案中选择最实惠的。最后，在实现代码中注意数组越界的问题。</p><p>使用dp[j]代表着我们旅行到i天为止需要的最少旅行价格，递推公式为：</p><ol type="1"><li>dp[j] = dp[j-1] （第j天不用旅行）</li><li>dp[j] = min(dp[j-1] + costs[0], dp[j-7] + costs[1], dp[j-30] + costs[2]) （第j天需要旅行）</li></ol><h2 id="c实现">C++实现</h2><div class="sourceCode" id="cb1"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw">class</span> Solution {</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">public</span>:</a><a class="sourceLine" id="cb1-3" data-line-number="3">    <span class="dt">int</span> mincostTickets(vector&lt;<span class="dt">int</span>&gt; &amp;days, vector&lt;<span class="dt">int</span>&gt; &amp;costs) {</a><a class="sourceLine" id="cb1-4" data-line-number="4">        <span class="cf">if</span> (days.size() == <span class="dv">0</span>) <span class="cf">return</span> <span class="dv">0</span>;</a><a class="sourceLine" id="cb1-5" data-line-number="5">        assert(costs.size() == <span class="dv">3</span>);</a><a class="sourceLine" id="cb1-6" data-line-number="6">        <span class="co">// dp[i]代表着我们旅行到i天需要的最少旅行价格, dp[0]为0，没实际含义</span></a><a class="sourceLine" id="cb1-7" data-line-number="7">        array&lt;<span class="dt">int</span>, <span class="dv">366</span>&gt; dp = {<span class="dv">0</span>};</a><a class="sourceLine" id="cb1-8" data-line-number="8">        <span class="cf">for</span> (<span class="dt">int</span> i = <span class="dv">1</span>; i &lt; dp.size(); ++i) {</a><a class="sourceLine" id="cb1-9" data-line-number="9">            <span class="co">// 如果这一天不旅行</span></a><a class="sourceLine" id="cb1-10" data-line-number="10">            <span class="cf">if</span> (find(days.begin(), days.end(), i) == days.end()) dp[i] = dp[i - <span class="dv">1</span>];</a><a class="sourceLine" id="cb1-11" data-line-number="11">            <span class="cf">else</span> {</a><a class="sourceLine" id="cb1-12" data-line-number="12">                dp[i] = min({</a><a class="sourceLine" id="cb1-13" data-line-number="13">                   dp[i - <span class="dv">1</span>] + costs[<span class="dv">0</span>],</a><a class="sourceLine" id="cb1-14" data-line-number="14">                   dp[max(<span class="dv">0</span>, i - <span class="dv">7</span>)] + costs[<span class="dv">1</span>],</a><a class="sourceLine" id="cb1-15" data-line-number="15">                   dp[max(<span class="dv">0</span>, i - <span class="dv">30</span>)] + costs[<span class="dv">2</span>]</a><a class="sourceLine" id="cb1-16" data-line-number="16">                });</a><a class="sourceLine" id="cb1-17" data-line-number="17">            }</a><a class="sourceLine" id="cb1-18" data-line-number="18">        }</a><a class="sourceLine" id="cb1-19" data-line-number="19">        <span class="cf">return</span> dp[<span class="dv">365</span>];</a><a class="sourceLine" id="cb1-20" data-line-number="20">    }</a><a class="sourceLine" id="cb1-21" data-line-number="21">};</a></code></pre></div><h2 id="scala实现">Scala实现</h2><p>注：如果有童鞋有纯函数的实现，希望分享出来！共享！</p><div class="sourceCode" id="cb2"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">object</span> Solution {</a><a class="sourceLine" id="cb2-2" data-line-number="2">  <span class="kw">def</span> <span class="fu">mincostTickets</span>(days: Array[Int], costs: Array[Int]): Int = {</a><a class="sourceLine" id="cb2-3" data-line-number="3">    <span class="kw">if</span> (days.<span class="fu">length</span> == <span class="dv">0</span>) <span class="kw">return</span> <span class="dv">0</span></a><a class="sourceLine" id="cb2-4" data-line-number="4">    <span class="fu">assert</span>(costs.<span class="fu">length</span> == <span class="dv">3</span>)</a><a class="sourceLine" id="cb2-5" data-line-number="5">    <span class="kw">val</span> travels = days.<span class="fu">toSet</span></a><a class="sourceLine" id="cb2-6" data-line-number="6">    <span class="kw">val</span> dp = Array.<span class="fu">fill</span>[Int](<span class="dv">366</span>)(<span class="dv">0</span>)</a><a class="sourceLine" id="cb2-7" data-line-number="7"></a><a class="sourceLine" id="cb2-8" data-line-number="8">    <span class="kw">for</span> (i &lt;- <span class="dv">1</span> until <span class="dv">366</span>) {</a><a class="sourceLine" id="cb2-9" data-line-number="9">      <span class="kw">if</span> (!travels.<span class="fu">contains</span>(i)) <span class="fu">dp</span>(i) = <span class="fu">dp</span>(i - <span class="dv">1</span>)</a><a class="sourceLine" id="cb2-10" data-line-number="10">      <span class="kw">else</span> <span class="fu">dp</span>(i) = List(</a><a class="sourceLine" id="cb2-11" data-line-number="11">        <span class="fu">dp</span>(i - <span class="dv">1</span>) + <span class="fu">costs</span>(<span class="dv">0</span>),</a><a class="sourceLine" id="cb2-12" data-line-number="12">        <span class="fu">dp</span>(math.<span class="fu">max</span>(<span class="dv">0</span>, i - <span class="dv">7</span>)) + <span class="fu">costs</span>(<span class="dv">1</span>),</a><a class="sourceLine" id="cb2-13" data-line-number="13">        <span class="fu">dp</span>(math.<span class="fu">max</span>(<span class="dv">0</span>, i - <span class="dv">30</span>)) + <span class="fu">costs</span>(<span class="dv">2</span>)</a><a class="sourceLine" id="cb2-14" data-line-number="14">      ).<span class="fu">min</span></a><a class="sourceLine" id="cb2-15" data-line-number="15">    }</a><a class="sourceLine" id="cb2-16" data-line-number="16"></a><a class="sourceLine" id="cb2-17" data-line-number="17">    <span class="fu">dp</span>(<span class="dv">365</span>)</a><a class="sourceLine" id="cb2-18" data-line-number="18">  }</a><a class="sourceLine" id="cb2-19" data-line-number="19">}</a></code></pre></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;minimum-cost-for-tickets&quot;&gt;Minimum Cost For Tickets&lt;/h1&gt;
&lt;h2 id=&quot;题目描述&quot;&gt;题目描述&lt;/h2&gt;
&lt;p&gt;LeetCode地址：&lt;a href=&quot;https://leetcode.com/problems
      
    
    </summary>
    
      <category term="算法" scheme="http://theonegis.github.io/categories/algorithm/"/>
    
    
      <category term="LeetCode" scheme="http://theonegis.github.io/tags/LeetCode/"/>
    
      <category term="动态规划" scheme="http://theonegis.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>小波变换三之Haar变换</title>
    <link href="http://theonegis.github.io/math/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2%E4%B8%89%E4%B9%8BHaar%E5%8F%98%E6%8D%A2/"/>
    <id>http://theonegis.github.io/math/小波变换三之Haar变换/</id>
    <published>2019-01-29T03:27:41.000Z</published>
    <updated>2019-03-22T19:39:53.832Z</updated>
    
    <content type="html"><![CDATA[<h1 id="小波变换三之haar变换">小波变换三之Haar变换</h1><h2 id="什么是基basis">什么是基（Basis）</h2><p>数学上有一个常用神秘专有名词“基”，那么什么是“基”呢？举个例子：在平面直角坐标系中的的一个点<span class="math inline">\((x, y)\)</span>的坐标可以表示为<span class="math inline">\(x\cdot{(1, 0)} + y\cdot{(0, 1)}\)</span>，这里的<span class="math inline">\((1, 0)\)</span>和<span class="math inline">\((0, 1)​\)</span>就是二维直角坐标系中的基，因为任意的点都可以通过这两个向量的加权进行表示。</p><p>其实，数学中很多定理或者法则都有这样的表示形式。比如：泰勒公式将任意一个可微函数表示为在该函数在某点的各阶导数的多项式的和；傅里叶级数任何周期函数都可以用正弦函数和余弦函数构成的无穷级数来表示。这些定理都是用无穷项的和来毕竟一个函数，而无穷项中的每一项都是一个系数乘以一个给定的函数，这些函数一起构成了所谓的“基”。</p><h2 id="haar小波基">Haar小波基</h2><p>其实，小波变换也是有“基”的。我们先直观来看，然后给出形式化的定义。</p><p>看例子，对于一个信号<span class="math inline">\(f = \{4, 6, 10, 12, 8, 6, 5, 5\}\)</span>，我们可以通过在《<a href="https://theonegis.github.io/math/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2%E4%B8%80%E4%B9%8BHaar%E5%8F%98%E6%8D%A2/">小波变换一之Haar变换</a>》中讲述的方法计算其第一层的变换结果，我们也可以通过“基”辅助计算。</p><h3 id="第一层的基">第一层的基</h3><p>对于第一层的计算，Haar基是这样的：</p><p>对于近似表示的基，我们有：<span class="math display">\[\begin{matrix}V_1^1 = (\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, 0, 0, \cdots, 0) \\ V_2^1 = (0, 0, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}, \cdots, 0) \\ V_{N/2}^1 = (0, 0, 0, 0, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})\end{matrix}​\]</span></p><p>所以，变换以后的近似系数为<span class="math inline">\(a^1 = (fV_1^1, fV_2^1, \cdots, fV_{N/2}^1) = (5\sqrt{2}, 11\sqrt{2}, 7\sqrt{2}, 5\sqrt{2})​\)</span></p><p>类似的，对于细节表示的基，我们有：<span class="math display">\[\begin{matrix}W_1^1 = (\frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}, 0, 0, \cdots, 0) \\ W_2^1 = (0, 0, \frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}}, \cdots, 0) \\ W_{N/2}^1 = (0, 0, 0, 0, \frac{1}{\sqrt{2}}, -\frac{1}{\sqrt{2}})\end{matrix}​\]</span></p><p>所以，变换以后的细节系数为<span class="math inline">\(d^1 = (fW_1^1, fW_2^1, \cdots, fW_{N/2}^1) = (-\sqrt{2}, -\sqrt{2}, -\sqrt{2}, 0)\)</span></p><h3 id="第二层的基">第二层的基</h3><p>对于第二层的计算（对<span class="math inline">\(a_1\)</span>进行小波分解），Haar基是这样的：</p><p>对于近似表示的基，我们有：<span class="math display">\[\begin{matrix}V_1^2 = (\frac{1}{2}, \frac{1}{2}, \frac{1}{2}, \frac{1}{2},0, 0, 0, 0, \cdots, 0, 0, 0, 0) \\ V_2^2 = (0, 0, 0, 0, \frac{1}{2}, \frac{1}{2}, \frac{1}{2}, \frac{1}{2}, \cdots, 0, 0, 0, 0) \\ V_{N/4}^2 = (0, 0, 0, 0, 0, 0, 0, 0,  \cdots, \frac{1}{2}, \frac{1}{2}, \frac{1}{2}, \frac{1}{2})\end{matrix}​\]</span></p><p>变换以后的近似系数为<span class="math inline">\(a^2 = (fV_1^2, fV_2^2, \cdots, fV_{N/4}^2) = (16, 12)\)</span></p><p>对于细节表示的基，我们有：<span class="math display">\[\begin{matrix}W_1^2 = (\frac{1}{2}, \frac{1}{2}, -\frac{1}{2}, -\frac{1}{2},0, 0, 0, 0, \cdots, 0, 0, 0, 0) \\ W_2^2 = (0, 0, 0, 0, \frac{1}{2}, \frac{1}{2}, -\frac{1}{2}, -\frac{1}{2}, \cdots, 0, 0, 0, 0) \\ W_{N/4}^2 = (0, 0, 0, 0, 0, 0, 0, 0,  \cdots, \frac{1}{2}, \frac{1}{2}, -\frac{1}{2}, -\frac{1}{2})\end{matrix}\]</span></p><p>变换以后的细节系数为<span class="math inline">\(d^1 = (fW_1^2, fW_2^2, \cdots, fW_{N/4}^2) = (-6, 2)​\)</span></p><p>后面，如果要继续再分解的话，我们可以找到类似上面的“基”做进一步分解。</p><p>可以看到Haar小波基都是正交的（与除了自己以外的其它基的內积为0），而且都经过了单位化（模为1）。</p><h2 id="母小波和父小波">母小波和父小波</h2><p>在小波变换中有两个重要的术语：母小波（mother wavelet）和父小波（father wavelet），而我们的小波基就是由父小波和母小波经过平移和缩放得到的。母小波也叫做小波函数（wavelet function），对应着细节系数的基，父小波也叫做缩放函数（scaling function），对应着近似系数的基。</p><p>Haar小波的母小波定义为<span class="math display">\[\psi(x) = \begin{cases}1, &amp; 0 \le x \lt \frac{1}{2} \\-1, &amp; \frac{1}{2}\le x \lt 1\\ 0, &amp; \mathrm{其它}\end{cases}​\]</span></p><p>Haar小波的父小波定义为<span class="math display">\[\phi(x) = \begin{cases}1, &amp; 0 \le x \le 1\\ 0, &amp; \mathrm{其它}\end{cases}\]</span></p><p>不止对于Haar小波，任何小波的基都是对其母小波和父小波缩放和平移后的集合。感兴趣的朋友可以在下面的网址中查看一下，如何对小波函数进行缩放和平移：<a href="http://demonstrations.wolfram.com/HaarFunctions/" target="_blank" rel="noopener">Haar Functions</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;小波变换三之haar变换&quot;&gt;小波变换三之Haar变换&lt;/h1&gt;
&lt;h2 id=&quot;什么是基basis&quot;&gt;什么是基（Basis）&lt;/h2&gt;
&lt;p&gt;数学上有一个常用神秘专有名词“基”，那么什么是“基”呢？举个例子：在平面直角坐标系中的的一个点&lt;span class=&quot;m
      
    
    </summary>
    
      <category term="数学" scheme="http://theonegis.github.io/categories/math/"/>
    
    
      <category term="小波变换" scheme="http://theonegis.github.io/tags/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/"/>
    
      <category term="Haar变换" scheme="http://theonegis.github.io/tags/Haar%E5%8F%98%E6%8D%A2/"/>
    
      <category term="Wavelet" scheme="http://theonegis.github.io/tags/Wavelet/"/>
    
  </entry>
  
  <entry>
    <title>C++中的万能引用和完美转发</title>
    <link href="http://theonegis.github.io/cxx/C-%E4%B8%AD%E7%9A%84%E4%B8%87%E8%83%BD%E5%BC%95%E7%94%A8%E5%92%8C%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/"/>
    <id>http://theonegis.github.io/cxx/C-中的万能引用和完美转发/</id>
    <published>2019-01-20T09:38:11.000Z</published>
    <updated>2019-03-22T19:39:53.821Z</updated>
    
    <content type="html"><![CDATA[<h1 id="c中的万能引用和完美转发">C++中的万能引用和完美转发</h1><ol type="1"><li>阅读这篇博文需要了解C++中的左值（lvalue）和右值（rvalue）的概念，详情参见我的另外一篇博文：<a href="https://theonegis.github.io/cxx/C-%E7%A7%BB%E5%8A%A8%E8%AF%AD%E4%B9%89%E5%8F%8A%E6%8B%B7%E8%B4%9D%E4%BC%98%E5%8C%96/">C++移动语义及拷贝优化</a></li><li>万能引用和完美转发多涉及到模板的使用，如若不是自己写模板，则可不用关心</li></ol><h2 id="万能引用universal-reference">万能引用（Universal Reference）</h2><p>首先，我们来看一个例子：</p><div class="sourceCode" id="cb1"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="pp">#include </span><span class="im">&lt;iostream&gt;</span></a><a class="sourceLine" id="cb1-2" data-line-number="2"></a><a class="sourceLine" id="cb1-3" data-line-number="3"><span class="kw">using</span> <span class="bu">std::</span>cout;</a><a class="sourceLine" id="cb1-4" data-line-number="4"><span class="kw">using</span> <span class="bu">std::</span>endl;</a><a class="sourceLine" id="cb1-5" data-line-number="5"></a><a class="sourceLine" id="cb1-6" data-line-number="6"></a><a class="sourceLine" id="cb1-7" data-line-number="7"><span class="kw">template</span>&lt;<span class="kw">typename</span> T&gt;</a><a class="sourceLine" id="cb1-8" data-line-number="8"><span class="dt">void</span> func(T&amp; param) {</a><a class="sourceLine" id="cb1-9" data-line-number="9">    cout &lt;&lt; param &lt;&lt; endl;</a><a class="sourceLine" id="cb1-10" data-line-number="10">}</a><a class="sourceLine" id="cb1-11" data-line-number="11"></a><a class="sourceLine" id="cb1-12" data-line-number="12"></a><a class="sourceLine" id="cb1-13" data-line-number="13"><span class="dt">int</span> main() {</a><a class="sourceLine" id="cb1-14" data-line-number="14">    <span class="dt">int</span> num = <span class="dv">2019</span>;</a><a class="sourceLine" id="cb1-15" data-line-number="15">    func(num);</a><a class="sourceLine" id="cb1-16" data-line-number="16">    <span class="cf">return</span> <span class="dv">0</span>;</a><a class="sourceLine" id="cb1-17" data-line-number="17">}</a></code></pre></div><p>这样例子的编译输出都没有什么问题，但是如果我们修改成下面的调用方式呢？</p><div class="sourceCode" id="cb2"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="dt">int</span> main() {</a><a class="sourceLine" id="cb2-2" data-line-number="2">    func(<span class="dv">2019</span>);</a><a class="sourceLine" id="cb2-3" data-line-number="3">    <span class="cf">return</span> <span class="dv">0</span>;</a><a class="sourceLine" id="cb2-4" data-line-number="4">}</a></code></pre></div><p>则会得到一个大大的编译错误。因为上面的模板函数只能接受左值或者左值引用（左值一般是有名字的变量，可以取到地址的），我们当然可以重载一个接受右值的模板函数，如下也可以达到效果。</p><div class="sourceCode" id="cb3"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">template</span>&lt;<span class="kw">typename</span> T&gt;</a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="dt">void</span> func(T&amp; param) {</a><a class="sourceLine" id="cb3-3" data-line-number="3">    cout &lt;&lt; <span class="st">&quot;传入的是左值&quot;</span> &lt;&lt; endl;</a><a class="sourceLine" id="cb3-4" data-line-number="4">}</a><a class="sourceLine" id="cb3-5" data-line-number="5"><span class="kw">template</span>&lt;<span class="kw">typename</span> T&gt;</a><a class="sourceLine" id="cb3-6" data-line-number="6"><span class="dt">void</span> func(T&amp;&amp; param) {</a><a class="sourceLine" id="cb3-7" data-line-number="7">    cout &lt;&lt; <span class="st">&quot;传入的是右值&quot;</span> &lt;&lt; endl;</a><a class="sourceLine" id="cb3-8" data-line-number="8">}</a><a class="sourceLine" id="cb3-9" data-line-number="9"></a><a class="sourceLine" id="cb3-10" data-line-number="10"></a><a class="sourceLine" id="cb3-11" data-line-number="11"></a><a class="sourceLine" id="cb3-12" data-line-number="12"><span class="dt">int</span> main() {</a><a class="sourceLine" id="cb3-13" data-line-number="13">    <span class="dt">int</span> num = <span class="dv">2019</span>;</a><a class="sourceLine" id="cb3-14" data-line-number="14">    func(num);</a><a class="sourceLine" id="cb3-15" data-line-number="15">    func(<span class="dv">2019</span>);</a><a class="sourceLine" id="cb3-16" data-line-number="16">    <span class="cf">return</span> <span class="dv">0</span>;</a><a class="sourceLine" id="cb3-17" data-line-number="17">}</a></code></pre></div><p>输出结果：</p><pre><code>传入的是左值传入的是右值</code></pre><p>第一次函数调用的是左值得版本，第二次函数调用的是右值版本。但是，有没有办法只写一个模板函数即可以接收左值又可以接收右值呢？</p><p>C++ 11中有万能引用（Universal Reference）的概念：使用<code>T&amp;&amp;</code>类型的形参既能绑定右值，又能绑定左值。</p><p>但是注意了：<strong>只有发生类型推导的时候，T&amp;&amp;才表示万能引用</strong>；否则，表示右值引用。</p><p>所以，上面的案例我们可以修改为：</p><div class="sourceCode" id="cb5"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="kw">template</span>&lt;<span class="kw">typename</span> T&gt;</a><a class="sourceLine" id="cb5-2" data-line-number="2"><span class="dt">void</span> func(T&amp;&amp; param) {</a><a class="sourceLine" id="cb5-3" data-line-number="3">    cout &lt;&lt; param &lt;&lt; endl;</a><a class="sourceLine" id="cb5-4" data-line-number="4">}</a><a class="sourceLine" id="cb5-5" data-line-number="5"></a><a class="sourceLine" id="cb5-6" data-line-number="6"></a><a class="sourceLine" id="cb5-7" data-line-number="7"><span class="dt">int</span> main() {</a><a class="sourceLine" id="cb5-8" data-line-number="8">    <span class="dt">int</span> num = <span class="dv">2019</span>;</a><a class="sourceLine" id="cb5-9" data-line-number="9">    func(num);</a><a class="sourceLine" id="cb5-10" data-line-number="10">    func(<span class="dv">2019</span>);</a><a class="sourceLine" id="cb5-11" data-line-number="11">    <span class="cf">return</span> <span class="dv">0</span>;</a><a class="sourceLine" id="cb5-12" data-line-number="12">}</a></code></pre></div><h2 id="引用折叠universal-collapse">引用折叠（Universal Collapse）</h2><p>万能引用说完了，接着来聊引用折叠（Univers Collapse），因为完美转发（Perfect Forwarding）的概念涉及引用折叠。一个模板函数，根据定义的形参和传入的实参的类型，我们可以有下面四中组合：</p><ul><li>左值-左值 T&amp; &amp; # 函数定义的形参类型是左值引用，传入的实参是左值引用</li><li>左值-右值 T&amp; &amp;&amp; # 函数定义的形参类型是左值引用，传入的实参是右值引用</li><li>右值-左值 T&amp;&amp; &amp; # 函数定义的形参类型是右值引用，传入的实参是左值引用</li><li>右值-右值 T&amp;&amp; &amp;&amp; # 函数定义的形参类型是右值引用，传入的实参是右值引用</li></ul><p>但是C++中不允许对引用再进行引用，对于上述情况的处理有如下的规则：</p><p>所有的折叠引用最终都代表一个引用，要么是左值引用，要么是右值引用。规则是：<strong>如果任一引用为左值引用，则结果为左值引用。否则（即两个都是右值引用），结果为右值引用</strong>。</p><p>即就是前面三种情况代表的都是左值引用，而第四种代表的右值引用。</p><h2 id="完美转发perfect-forwarding">完美转发（Perfect Forwarding）</h2><p>下面接着说完美转发（Perfect Forwarding），首先，看一个例子：</p><div class="sourceCode" id="cb6"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="pp">#include </span><span class="im">&lt;iostream&gt;</span></a><a class="sourceLine" id="cb6-2" data-line-number="2"></a><a class="sourceLine" id="cb6-3" data-line-number="3"><span class="kw">using</span> <span class="bu">std::</span>cout;</a><a class="sourceLine" id="cb6-4" data-line-number="4"><span class="kw">using</span> <span class="bu">std::</span>endl;</a><a class="sourceLine" id="cb6-5" data-line-number="5"></a><a class="sourceLine" id="cb6-6" data-line-number="6"><span class="kw">template</span>&lt;<span class="kw">typename</span> T&gt;</a><a class="sourceLine" id="cb6-7" data-line-number="7"><span class="dt">void</span> func(T&amp; param) {</a><a class="sourceLine" id="cb6-8" data-line-number="8">    cout &lt;&lt; <span class="st">&quot;传入的是左值&quot;</span> &lt;&lt; endl;</a><a class="sourceLine" id="cb6-9" data-line-number="9">}</a><a class="sourceLine" id="cb6-10" data-line-number="10"><span class="kw">template</span>&lt;<span class="kw">typename</span> T&gt;</a><a class="sourceLine" id="cb6-11" data-line-number="11"><span class="dt">void</span> func(T&amp;&amp; param) {</a><a class="sourceLine" id="cb6-12" data-line-number="12">    cout &lt;&lt; <span class="st">&quot;传入的是右值&quot;</span> &lt;&lt; endl;</a><a class="sourceLine" id="cb6-13" data-line-number="13">}</a><a class="sourceLine" id="cb6-14" data-line-number="14"></a><a class="sourceLine" id="cb6-15" data-line-number="15"></a><a class="sourceLine" id="cb6-16" data-line-number="16"><span class="kw">template</span>&lt;<span class="kw">typename</span> T&gt;</a><a class="sourceLine" id="cb6-17" data-line-number="17"><span class="dt">void</span> warp(T&amp;&amp; param) {</a><a class="sourceLine" id="cb6-18" data-line-number="18">    func(param);</a><a class="sourceLine" id="cb6-19" data-line-number="19">}</a><a class="sourceLine" id="cb6-20" data-line-number="20"></a><a class="sourceLine" id="cb6-21" data-line-number="21"></a><a class="sourceLine" id="cb6-22" data-line-number="22"><span class="dt">int</span> main() {</a><a class="sourceLine" id="cb6-23" data-line-number="23">    <span class="dt">int</span> num = <span class="dv">2019</span>;</a><a class="sourceLine" id="cb6-24" data-line-number="24">    warp(num);</a><a class="sourceLine" id="cb6-25" data-line-number="25">    warp(<span class="dv">2019</span>);</a><a class="sourceLine" id="cb6-26" data-line-number="26">    <span class="cf">return</span> <span class="dv">0</span>;</a><a class="sourceLine" id="cb6-27" data-line-number="27">}</a></code></pre></div><p>猜一下，上面的输出结果是什么？</p><pre><code>传入的是左值传入的是左值</code></pre><p>是不是和我们预期的不一样，下面我们来分析一下原因：</p><p><code>warp()</code>函数本身的形参是一个万能引用，即可以接受左值又可以接受右值；第一个<code>warp()</code>函数调用实参是左值，所以，<code>warp()</code>函数中调用<code>func()</code>中传入的参数也应该是左值；第二个<code>warp()</code>函数调用实参是右值，根据上面所说的引用折叠规则，warp()<code>函数接收的参数类型是右值引用，那么为什么却调用了调用</code>func()的左值版本了呢？这是因为在<code>warp()</code>函数内部，左值引用类型变为了右值，因为参数有了名称，我们也通过变量名取得变量地址。</p><p>那么问题来了，怎么保持函数调用过程中，变量类型的不变呢？这就是我们所谓的“完美转发”技术，在C++11中通过<code>std::forward()</code>函数来实现。我们修改我们的<code>warp()</code>函数如下：</p><div class="sourceCode" id="cb8"><pre class="sourceCode c++"><code class="sourceCode cpp"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="kw">template</span>&lt;<span class="kw">typename</span> T&gt;</a><a class="sourceLine" id="cb8-2" data-line-number="2"><span class="dt">void</span> warp(T&amp;&amp; param) {</a><a class="sourceLine" id="cb8-3" data-line-number="3">    func(<span class="bu">std::</span>forward&lt;T&gt;(param));</a><a class="sourceLine" id="cb8-4" data-line-number="4">}</a></code></pre></div><p>则可以输出预期的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;c中的万能引用和完美转发&quot;&gt;C++中的万能引用和完美转发&lt;/h1&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;阅读这篇博文需要了解C++中的左值（lvalue）和右值（rvalue）的概念，详情参见我的另外一篇博文：&lt;a href=&quot;https://theonegis.
      
    
    </summary>
    
      <category term="C++" scheme="http://theonegis.github.io/categories/cxx/"/>
    
    
      <category term="C++11" scheme="http://theonegis.github.io/tags/C-11/"/>
    
      <category term="完美转发" scheme="http://theonegis.github.io/tags/%E5%AE%8C%E7%BE%8E%E8%BD%AC%E5%8F%91/"/>
    
      <category term="万能引用" scheme="http://theonegis.github.io/tags/%E4%B8%87%E8%83%BD%E5%BC%95%E7%94%A8/"/>
    
      <category term="引用折叠" scheme="http://theonegis.github.io/tags/%E5%BC%95%E7%94%A8%E6%8A%98%E5%8F%A0/"/>
    
  </entry>
  
  <entry>
    <title>小波变换二之Haar变换</title>
    <link href="http://theonegis.github.io/math/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2%E4%BA%8C%E4%B9%8BHaar%E5%8F%98%E6%8D%A2/"/>
    <id>http://theonegis.github.io/math/小波变换二之Haar变换/</id>
    <published>2019-01-17T06:45:55.000Z</published>
    <updated>2019-03-22T19:39:53.832Z</updated>
    
    <content type="html"><![CDATA[<h1 id="haar变换">Haar变换</h1><p>这是小波变换的第二篇，我们继续谈Haar变换。在第一篇中，我们介绍了一位情况下的Haar变换，这篇博文中主要介绍二维Haar变换。最后，通过一个图像压缩的案例说明二维Haar变换的应用。</p><h2 id="原理说明">原理说明</h2><p>给定一个二维信号，我们这里假设是一个<span class="math inline">\(4\times4\)</span>的图片，</p><p><span class="math inline">\(f=\begin{bmatrix}2&amp;1&amp;5&amp;6\\7&amp;6&amp;5&amp;8\\2&amp;1&amp;5&amp;5\\7&amp;7&amp;2&amp;10\end{bmatrix}\)</span></p><p>如何进行二维的哈尔变换呢？</p><p>步骤是这样的：（1）首先，沿着矩阵的每一行做一维的Haar变换；（2）然后，沿着矩阵的每一列做一维的哈尔变换；（3）对于每个低频分量矩阵（近似信息）重复步骤（1）和（2）直到完成指定的等级划分。下图给出了两级划分的示意图：</p><figure><img src="/images/math/Haar2D.png" alt="二维Haar变换示意图"><figcaption>二维Haar变换示意图</figcaption></figure><p>这里的A表示近似信息（approximation coefficients），H表示水平细节信息（horizontal detail coefficients），V表示垂直细节信息（vertical detail coefficients），D表示对角线细节信息（diagonal detail coefficients）。很多数学软件中是这样称呼的，了解了这个可以帮助我们快速上手软件进行实际操作。</p><p>行分解和列分解的顺序是可以互换的，保持一致即可。</p><p>明白了基本原理，下面我们来进行实际计算，对于<span class="math inline">\(f\)</span>，（如果不清楚如何做一维高频和低频分解，可参看博文<a href="https://blog.csdn.net/theonegis/article/details/86517377" target="_blank" rel="noopener">《小波变换一之Haar变换》</a>）</p><p>第一次行分解得到低频信息<span class="math inline">\(L=\begin{bmatrix}\frac{3}{\sqrt{2}}&amp;\frac{11}{\sqrt{2}}\\\frac{13}{\sqrt{2}}&amp;\frac{13}{\sqrt{2}}\\\frac{3}{\sqrt{2}}&amp;5\sqrt{2}\\7\sqrt{2}&amp;6\sqrt{2}\end{bmatrix}\)</span></p><p>第一次列分解得到高频信息<span class="math inline">\(H=\begin{bmatrix}\frac{1}{\sqrt{2}}&amp;-\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}&amp;-\frac{3}{\sqrt{2}}\\\frac{1}{\sqrt{2}}&amp;0\\0&amp;-4\sqrt{2}\end{bmatrix}\)</span></p><p>对<span class="math inline">\(L\)</span>进行列高频分解得到<span class="math inline">\(A_1=\begin{bmatrix}8&amp;12\\8.5&amp;11\end{bmatrix}\)</span></p><p>对<span class="math inline">\(L\)</span>进行列低频分解得到<span class="math inline">\(H_1=\begin{bmatrix}-5&amp;-1\\-5.5&amp;-1\end{bmatrix}\)</span></p><p>对<span class="math inline">\(H\)</span>进行列高频分解得到<span class="math inline">\(V_1=\begin{bmatrix}1&amp;-2\\0.5&amp;-4\end{bmatrix}\)</span></p><p>对<span class="math inline">\(H\)</span>进行列低频分解得到<span class="math inline">\(D_1=\begin{bmatrix}0&amp;1\\0.5&amp;4\end{bmatrix}​\)</span></p><p>我们还可以对<span class="math inline">\(A_1​\)</span>继续进行二层分解，这里就不做演示了。</p><h2 id="实例演示">实例演示</h2><p>这里我们通过对一张图片做Haar变换，然后我们去掉其高频信息部分，实现对图像的压缩。</p><p>下面是进行了三次分解，然后分别过了到第一层的高频信息和第一层兼第二层的高频信息的效果！过滤掉第一层的高频信息，图像压缩为原来的四分之一，可以看到图像还是基本清晰的。过滤掉第二层和第二层的高频信息以后，可以看到图片稍微有点模糊了。</p><figure><img src="/images/math/Haar-Compress.jpg" alt="Haar变换实现图像压缩"><figcaption>Haar变换实现图像压缩</figcaption></figure><h2 id="matlab实现">MATLAB实现</h2><p>下面是使用MATLAB实现上面变换的代码，有兴趣的童鞋可以参考一下。</p><div class="sourceCode" id="cb1"><pre class="sourceCode matlab"><code class="sourceCode matlab"><a class="sourceLine" id="cb1-1" data-line-number="1">clear, clc;</a><a class="sourceLine" id="cb1-2" data-line-number="2"></a><a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co">% 读取原始图像</span></a><a class="sourceLine" id="cb1-4" data-line-number="4">X = rgb2gray(imread(<span class="st">&#39;http://www.lenna.org/lena_std.tif&#39;</span>));</a><a class="sourceLine" id="cb1-5" data-line-number="5"><span class="co">% 进行小波分解</span></a><a class="sourceLine" id="cb1-6" data-line-number="6">[C, S] = wavedec2(X, <span class="fl">3</span>, <span class="st">&#39;haar&#39;</span>);</a><a class="sourceLine" id="cb1-7" data-line-number="7"></a><a class="sourceLine" id="cb1-8" data-line-number="8"><span class="co">% 获得分解以后的低频近似信息</span></a><a class="sourceLine" id="cb1-9" data-line-number="9">L = appcoef2(C, S, <span class="st">&#39;haar&#39;</span>, <span class="fl">3</span>);</a><a class="sourceLine" id="cb1-10" data-line-number="10"><span class="co">% 分别获得各层级的高频细节信息</span></a><a class="sourceLine" id="cb1-11" data-line-number="11">[H3, V3, D3] = detcoef2(<span class="st">&#39;all&#39;</span>, C, S, <span class="fl">3</span>);</a><a class="sourceLine" id="cb1-12" data-line-number="12">[H2, V2, D2] = detcoef2(<span class="st">&#39;all&#39;</span>, C, S, <span class="fl">2</span>);</a><a class="sourceLine" id="cb1-13" data-line-number="13">[H1, V1, D1] = detcoef2(<span class="st">&#39;all&#39;</span>, C, S, <span class="fl">1</span>);</a><a class="sourceLine" id="cb1-14" data-line-number="14"></a><a class="sourceLine" id="cb1-15" data-line-number="15"><span class="co">% 去掉第一层的高频信息（替换成0），然后进行小波重建</span></a><a class="sourceLine" id="cb1-16" data-line-number="16"><span class="co">% 注意这里乘以3是有HVD三种高频信息</span></a><a class="sourceLine" id="cb1-17" data-line-number="17">D = [C(<span class="fl">1</span>: end - <span class="fl">3</span>*size(H1, <span class="fl">1</span>)*size(H1, <span class="fl">2</span>)), zeros(<span class="fl">1</span>, <span class="fl">3</span>*size(H1, <span class="fl">1</span>)*size(H1, <span class="fl">2</span>))];</a><a class="sourceLine" id="cb1-18" data-line-number="18">CD1 = waverec2(D, S, <span class="st">&#39;haar&#39;</span>);</a><a class="sourceLine" id="cb1-19" data-line-number="19"><span class="co">% 去掉第一和第二层的高频信息，然后进行小波重建</span></a><a class="sourceLine" id="cb1-20" data-line-number="20">D = [C(<span class="fl">1</span>: end - <span class="fl">3</span>*size(H1, <span class="fl">1</span>)*size(H1, <span class="fl">2</span>) - <span class="fl">3</span>*size(H2, <span class="fl">1</span>)*size(H2, <span class="fl">2</span>)), ...</a><a class="sourceLine" id="cb1-21" data-line-number="21">    zeros(<span class="fl">1</span>, <span class="fl">3</span>*size(H1, <span class="fl">1</span>)*size(H1, <span class="fl">2</span>) + <span class="fl">3</span>*size(H2, <span class="fl">1</span>)*size(H2, <span class="fl">2</span>))];</a><a class="sourceLine" id="cb1-22" data-line-number="22">CD2 = waverec2(D, S, <span class="st">&#39;haar&#39;</span>);</a><a class="sourceLine" id="cb1-23" data-line-number="23"></a><a class="sourceLine" id="cb1-24" data-line-number="24"><span class="co">%按照分解层级将分解系数排列拼接为一副图像</span></a><a class="sourceLine" id="cb1-25" data-line-number="25">DD1 = [L, H3; V3, D3];</a><a class="sourceLine" id="cb1-26" data-line-number="26">DD2 = [DD1, H2; V2, D2];</a><a class="sourceLine" id="cb1-27" data-line-number="27">DD3 = [DD2, H1; V1, D1];</a><a class="sourceLine" id="cb1-28" data-line-number="28"><span class="co">% 结果显示</span></a><a class="sourceLine" id="cb1-29" data-line-number="29">subplot(<span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">1</span>), imshow(X, []), title(<span class="st">&#39;原始图像&#39;</span>);</a><a class="sourceLine" id="cb1-30" data-line-number="30">subplot(<span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">2</span>), imshow(DD3, []), title(<span class="st">&#39;小波分解系数&#39;</span>);</a><a class="sourceLine" id="cb1-31" data-line-number="31">subplot(<span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">3</span>), imshow(CD1, []), title(<span class="st">&#39;压缩一（去掉第一层高频信息）&#39;</span>);</a><a class="sourceLine" id="cb1-32" data-line-number="32">subplot(<span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">4</span>), imshow(CD2, []), title(<span class="st">&#39;压缩二（去掉第二层高频信息）&#39;</span>);</a></code></pre></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;haar变换&quot;&gt;Haar变换&lt;/h1&gt;
&lt;p&gt;这是小波变换的第二篇，我们继续谈Haar变换。在第一篇中，我们介绍了一位情况下的Haar变换，这篇博文中主要介绍二维Haar变换。最后，通过一个图像压缩的案例说明二维Haar变换的应用。&lt;/p&gt;
&lt;h2 id=&quot;原理说
      
    
    </summary>
    
      <category term="数学" scheme="http://theonegis.github.io/categories/math/"/>
    
    
      <category term="小波变换" scheme="http://theonegis.github.io/tags/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/"/>
    
      <category term="Haar变换" scheme="http://theonegis.github.io/tags/Haar%E5%8F%98%E6%8D%A2/"/>
    
      <category term="图像压缩" scheme="http://theonegis.github.io/tags/%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>小波变换一之Haar变换</title>
    <link href="http://theonegis.github.io/math/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2%E4%B8%80%E4%B9%8BHaar%E5%8F%98%E6%8D%A2/"/>
    <id>http://theonegis.github.io/math/小波变换一之Haar变换/</id>
    <published>2019-01-16T06:17:30.000Z</published>
    <updated>2019-03-22T19:39:53.832Z</updated>
    
    <content type="html"><![CDATA[<p>注：</p><ul><li>小波变换系列博文打算记录自己学习小波变换的心路历程，每篇博文尽量简短，宗旨是用最少的数学公式说明白如何使用小波变换</li><li>我的博客即将同步至腾讯云+社区，邀请大家一同入驻：<a href="https://cloud.tencent.com/developer/support-plan?invite_code=1roiym8d609t1" class="uri" target="_blank" rel="noopener">https://cloud.tencent.com/developer/support-plan?invite_code=1roiym8d609t1</a></li></ul><h1 id="haar变换">Haar变换</h1><h2 id="案例一简单一维信号变换">案例一简单一维信号变换</h2><p>下面是一个一维信号（一组数）：<span class="math inline">\(f = \{2, 2, 2, 4, 4, 4\}\)</span></p><p>我对这个信号进行如下处理：</p><p><span class="math inline">\(a_m = \sqrt{2}\frac{f_{2m-1}+f_{2m}}{2} = \frac{f_{2m-1}+f_{2m}}{\sqrt{2}}\)</span>（相邻两个数相加，求平均，然后乘以<span class="math inline">\(\sqrt{2}\)</span>）</p><p><span class="math inline">\(d_m = \sqrt{2}\frac{f_{2m-1}-f_{2m}}{2} = \frac{f_{2m-1}-f_{2m}}{\sqrt{2}}\)</span>（相邻两个数相减，求平均，然后乘以<span class="math inline">\(\sqrt{2}\)</span>）</p><p>注：至于为什么要乘以<span class="math inline">\(\sqrt{2}\)</span>呢？我们这里先不解释，放到后面再说。</p><p>然后按照先<span class="math inline">\(a\)</span>后<span class="math inline">\(d\)</span>的顺序排列<span class="math inline">\({a_1,a_2,...,a_{N/2}, d_1, d_2, ..., d_{N/2}}\)</span>（<span class="math inline">\(N\)</span>是离散信号中的值的个数）</p><p>则，<span class="math inline">\(a = \{2\sqrt{2}, 3\sqrt{2}, 4\sqrt{2}\}\)</span>，<span class="math inline">\(d=\{0, -\sqrt{2}, 0\}\)</span></p><p>我们可以得到结果：<span class="math inline">\(tf = \{2\sqrt{2}, 3\sqrt{2}, 4\sqrt{2}, 0, -\sqrt{2}, 0\}\)</span></p><p>这就是传说中的Haar变换了……</p><p><span class="math inline">\(a\)</span>表示的是信号的趋势（trend），近似（approximation），是低频信息；而<span class="math inline">\(d\)</span>表示的是信号的细节（detail），是高频信息。</p><p>那么我们怎么变回去呢？我们对变换以后的信号进行如下处理：</p><p><span class="math inline">\(f_{2m-1} = \sqrt{2}\frac{a_m +d_m}{2} = \frac{a_m +d_m}{\sqrt{2}}\)</span>（第<span class="math inline">\(m\)</span>个<span class="math inline">\(a\)</span>和<span class="math inline">\(d\)</span>相加，求平均，然后乘以<span class="math inline">\(\sqrt{2}\)</span>）</p><p><span class="math inline">\(f_{2m} = \sqrt{2}\frac{a_m -d_m}{2} = \frac{a_m -d_m}{\sqrt{2}}\)</span> （第<span class="math inline">\(m\)</span>个<span class="math inline">\(a\)</span>和<span class="math inline">\(d\)</span>相减，求平均，然后乘以<span class="math inline">\(\sqrt{2}\)</span>）</p><p>我们可以得到结果<span class="math inline">\(if = \{2, 2, 2, 4, 4, 4\}\)</span></p><p>这样就是Haar变换的逆变换。</p><p>通过观察，我们可以发现：</p><ul><li><span class="math inline">\(d\)</span>中的数字绝大部分都很小（这是做信息压缩很重要的依据）</li><li>变换前后信号的能量保持不变，即<span class="math inline">\(\sum{f_i^2} = \sum{a_m^2} + \sum{d_i^2}\)</span>（有兴趣的同学可以算一下对于<span class="math inline">\(f\)</span>和<span class="math inline">\(tf\)</span>的能量都是60，刚好相等）</li></ul><h2 id="案例二多分辨率一维信号变换">案例二多分辨率一维信号变换</h2><p>我们可以按照上面的思路将信号对得到的低频信号（<span class="math inline">\(a\)</span>）一直一直划分下去，直到<span class="math inline">\(\mathrm{log}_2N\)</span>（离散信号的值的数目不是偶数的，可以在后面补0）</p><p>给定如下的一个信号：<span class="math inline">\(f(t) = 20x^2(1-x)^4\cos(12\pi x)\)</span></p><p>我们通过在[0, 1]之间取样1024个点可以得到信号的振幅，绘制出信号图像如下：</p><figure><img src="/images/math/原始信号.png" alt="原始信号"><figcaption>原始信号</figcaption></figure><p>我们可以通过案例一种描述的方法进行Haar变换，我们这里对<span class="math inline">\(f(t)\)</span>信号进行两次Haar变换，如下图所示：</p><figure><img src="/images/math/Haar多分辨率.png" alt="Haar多分辨率分析"><figcaption>Haar多分辨率分析</figcaption></figure><p>这是多分辨率分析（Multi-Resolution Analysis，MRA）以及图像压缩（JPEG2000编码）等的基础理念，这里现有一个大概理解，后面我们会继续谈到。</p><p>变换的结果如下（感兴趣的朋友可以使用Mathematica或者MATLAB是一样，这两个数学软件都提供了对Haar变换的直接支持）：</p><figure><img src="/images/math/Haar变换.png" alt="Haar变换"><figcaption>Haar变换</figcaption></figure><p>好了，这一节先到这里，我们以后有时间慢慢聊！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;注：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;小波变换系列博文打算记录自己学习小波变换的心路历程，每篇博文尽量简短，宗旨是用最少的数学公式说明白如何使用小波变换&lt;/li&gt;
&lt;li&gt;我的博客即将同步至腾讯云+社区，邀请大家一同入驻：&lt;a href=&quot;https://cloud.tencen
      
    
    </summary>
    
      <category term="数学" scheme="http://theonegis.github.io/categories/math/"/>
    
    
      <category term="小波变换" scheme="http://theonegis.github.io/tags/%E5%B0%8F%E6%B3%A2%E5%8F%98%E6%8D%A2/"/>
    
      <category term="Haar变换" scheme="http://theonegis.github.io/tags/Haar%E5%8F%98%E6%8D%A2/"/>
    
  </entry>
  
  <entry>
    <title>变分法入门介绍</title>
    <link href="http://theonegis.github.io/math/%E5%8F%98%E5%88%86%E6%B3%95%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"/>
    <id>http://theonegis.github.io/math/变分法入门介绍/</id>
    <published>2019-01-09T08:18:23.000Z</published>
    <updated>2019-03-22T19:39:53.831Z</updated>
    
    <content type="html"><![CDATA[<h1 id="变分法入门介绍">变分法入门介绍</h1><p>读完这篇博文你可以了解变分的基本概念，以及使用变分法求解最简泛函的极值。本文没有严密的数学证明，只是感性地对变分法做一个初步了解。</p><h2 id="泛函和变分法">泛函和变分法</h2><p>给定两点<span class="math inline">\(A(x_0, y_0)\)</span>和<span class="math inline">\(B(x_1, y_1)\)</span>，求AB两点之间的最短距离。两点之间直线最短，这还用球吗？可是为什么是直线最短呢，而不是其它曲线？</p><p>设链接AB两点的曲线为<span class="math inline">\(f(x)\)</span>,则AB之间的距离可以表示为在区间<span class="math inline">\([x_0, x_1]\)</span>上求<span class="math inline">\(\Delta{S}=\sqrt{(\Delta{x})^2 + (\Delta{y})^2}\)</span>线段的累积长度（积分的思想）：</p><p><span class="math display">\[S=\int_{x_0}^{x_1}\sqrt{1+f&#39;(x)^2}dx\]</span></p><p>在这里该函数的变量是<span class="math inline">\(f\)</span>，即函数的变量为函数，我们需要求解出合适的<span class="math inline">\(f\)</span>使得<span class="math inline">\(S\)</span>最小。我们把这样的函数<span class="math inline">\(S\)</span>称为泛函数。</p><p>定义：<strong>泛函是以函数为变量的函数。</strong></p><p>那么什么是变分法呢？<strong>求泛函极值的方法称为变分法。</strong></p><h2 id="变分法求泛函极值">变分法求泛函极值</h2><h3 id="变分的定义">变分的定义</h3><p>下面给出变分的定义：对于任意定值<span class="math inline">\(x\in [x_0, x_1]\)</span>，可取函数<span class="math inline">\(y(x)\)</span>与另一可取函数<span class="math inline">\(y_0(x)\)</span>之差<span class="math inline">\(y(x) - y_0(x)\)</span>称为函数<span class="math inline">\(y(x)\)</span>在<span class="math inline">\(y_0(x)\)</span>处的变分或函数的变分，记做<span class="math inline">\(\delta{y}\)</span>，这时有<span class="math inline">\(\delta{y}=y(x) - y_0(x)=\epsilon\eta(x)\)</span>，<span class="math inline">\(\epsilon\)</span>是一个很小的数，<span class="math inline">\(\eta(x)\)</span>是<span class="math inline">\(x\)</span>的任意参数</p><p>对于泛函<span class="math inline">\(J[y(x)]\)</span>的增量<span class="math inline">\(\Delta{J}=J[y(x)+\delta{y}] - J[y(x)] = \delta{J} + \mathcal{o}(\delta{y})\)</span></p><p>泛函的增量<span class="math inline">\(\Delta{J}\)</span>与变分<span class="math inline">\(\delta{J}\)</span>之差是一个比一阶距离更高阶的无穷小，泛函的变分是泛函增量的线性主要部分。</p><p>变分的定义是不是跟微分很像（微分的定义<span class="math inline">\(\Delta{y}=A\Delta{x}+\mathcal{o}(\Delta{x})=dy+\mathcal{o}(\Delta(x)\)</span>，<span class="math inline">\(A\)</span>是该点的导数）。类比一下，我们在高等数学中学习到的函数极值的必要条件是函数导数等于0，而泛函极值的必要条件也是泛函的变分等于0。</p><p>所以有如下定理：若泛函<span class="math inline">\(J[y(x)]\)</span>在<span class="math inline">\(y=y(x)\)</span>上达到极值，则它在<span class="math inline">\(y=y(x)\)</span>上的变分<span class="math inline">\(\delta{J}\)</span>等于零。这就是变分原理。</p><h3 id="拉格朗日函数">拉格朗日函数</h3><p>设<span class="math inline">\(F(x, y(x), y&#39;(x))\)</span>是三个独立变量<span class="math inline">\(x\)</span>，<span class="math inline">\(y(x)\)</span>，<span class="math inline">\(y&#39;(x)\)</span>在区间<span class="math inline">\([x_0, x_1]\)</span>上的已知函数，且二阶连续可微，其中<span class="math inline">\(y(x)\)</span>和<span class="math inline">\(y&#39;(x)\)</span>是<span class="math inline">\(x\)</span>的未知函数，则泛函</p><p><span class="math display">\[J[y(x)]=\int_{x_0}^{x_1}F(x, y(x), y&#39;(x))dx\]</span></p><p>称为最简单的积分形泛函，简称最简泛函，被积函数<span class="math inline">\(F\)</span>称为拉格朗日函数。</p><p>对于拉格朗日函数，其泛函的变分为</p><p><span class="math display">\[\delta{J} = \int_{x_0}^{x_1}(F_y\delta{y} +F_{y&#39;}\delta{y&#39;})dx = \int_{x_0}^{x_1}(F_y\delta{y})dx + (F_{y&#39;}\delta_{y}|_{x_0}^{x_1} - \int_{x_0}^{x_1}(\delta_{y}\frac{d}{dx}F_{y&#39;}d{x})=\int_{x_0}^{x_1}(F_y-\frac{d}{dx}F_{y&#39;})\delta{y}dx\]</span></p><h3 id="欧拉方程">欧拉方程</h3><p>利用变分原理，使最简泛函<span class="math inline">\(J[y(x)]=\int_{x_0}^{x_1}F(x, y(x), y&#39;(x))dx\)</span>取得极值且满足固定边界条件<span class="math inline">\(y(x_0)=y_0\)</span>，<span class="math inline">\(y(x_1)=y_1\)</span>的极值曲线<span class="math inline">\(y=y(x)\)</span>应满足必要条件</p><p><span class="math display">\[F_y-\frac{d}{dx}F_{y&#39;}=0\]</span></p><p>式中<span class="math inline">\(F\)</span>是<span class="math inline">\(x, y, y&#39;\)</span>的已知函数并有二阶连续偏导数。上述必要条件中的方程叫做泛函的欧拉方程，也叫欧拉-拉格朗日方程。而<span class="math inline">\(F_y-\frac{d}{dx}F_{y&#39;}\)</span>称为<span class="math inline">\(F\)</span>关于<span class="math inline">\(y\)</span>的变分导（函）数。</p><h2 id="案例分析两点之间直线最短">案例分析–两点之间直线最短</h2><p>好的，我们利用欧拉方程来证明博文刚开始提出的两点之间直线最短的问题。</p><p>这里的<span class="math inline">\(F=\sqrt{1+f&#39;(x)^2}\)</span>，求得<span class="math inline">\(F_y=0\)</span>，<span class="math inline">\(F_{y&#39;}=\frac{y&#39;}{\sqrt{1+{y&#39;}^2}}\)</span>，再求得<span class="math inline">\(\frac{d}{dx}F_{y&#39;}=y&#39;&#39;(1+{y&#39;}^2)^{-\frac{3}{2}}\)</span></p><p>根据欧拉方程有<span class="math inline">\(-y&#39;&#39;(1+{y&#39;}^2)^{-\frac{3}{2}}=0\)</span>，则<span class="math inline">\(y&#39;&#39;=0 \Rightarrow y&#39;=C \Rightarrow y=C_1x + C_2\)</span></p><p>此时，我们就得到了这条曲线确实就是连接两点的直线。</p><h2 id="在mathematica中使用变分法">在Mathematica中使用变分法</h2><p>鉴于本人计算能力超级差，手动求导对我来说实在太痛苦了，我将上述的计算借助于Mathematica计算了一遍，下面是计算过程。不得不说Mathematica真的太强大了。</p><figure><img src="/images/math/Mathematica-Variational.png" alt="Mathematica变分法"><figcaption>Mathematica变分法</figcaption></figure><h2 id="参考文献">参考文献</h2><p>老大中. 变分法基础[M]. 北京: 国防工业出版社. 2004.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;变分法入门介绍&quot;&gt;变分法入门介绍&lt;/h1&gt;
&lt;p&gt;读完这篇博文你可以了解变分的基本概念，以及使用变分法求解最简泛函的极值。本文没有严密的数学证明，只是感性地对变分法做一个初步了解。&lt;/p&gt;
&lt;h2 id=&quot;泛函和变分法&quot;&gt;泛函和变分法&lt;/h2&gt;
&lt;p&gt;给定两点&lt;s
      
    
    </summary>
    
      <category term="数学" scheme="http://theonegis.github.io/categories/math/"/>
    
    
      <category term="变分法" scheme="http://theonegis.github.io/tags/%E5%8F%98%E5%88%86%E6%B3%95/"/>
    
      <category term="泛函" scheme="http://theonegis.github.io/tags/%E6%B3%9B%E5%87%BD/"/>
    
  </entry>
  
  <entry>
    <title>相关系数r和决定系数R2的那些事</title>
    <link href="http://theonegis.github.io/math/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0r%E5%92%8C%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0R2%E7%9A%84%E9%82%A3%E4%BA%9B%E4%BA%8B/"/>
    <id>http://theonegis.github.io/math/相关系数r和决定系数R2的那些事/</id>
    <published>2019-01-06T12:27:07.000Z</published>
    <updated>2019-03-22T19:39:53.835Z</updated>
    
    <content type="html"><![CDATA[<h1 id="相关系数r和决定系数r2的那些事">相关系数<span class="math inline">\(r\)</span>和决定系数<span class="math inline">\(R^2\)</span>的那些事</h1><p>有人说相关系数（correlation coefficient，<span class="math inline">\(r\)</span>）和决定系数（coefficient of determination，<span class="math inline">\(R^2\)</span>，读作R-Squared）都是评价两个变量相关性的指标，且相关系数的平方就是决定系数？这种说法对不对呢？请听下文分解！</p><h2 id="协方差与相关系数">协方差与相关系数</h2><p>要说相关系数，我们先来聊聊协方差。在之前的博文《<a href="https://blog.csdn.net/theonegis/article/details/85059105" target="_blank" rel="noopener">使用Python计算方差协方差相关系数</a>》中提到协方差是计算两个随机变量<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span> 之间的相关性的指标，定义如下：</p><p><span class="math display">\[\mathrm{Cov}(X, Y) = \mathrm{E}[(X - \mathrm{E}X)(Y - \mathrm{E}Y)]\]</span></p><p>但是协方差有一个确定：它的值会随着变量量纲的变化而变化（covariance is not scale invariant），所以，这才提出了相关系数的概念：</p><p><span class="math display">\[r = \mathrm{Corr}(X, Y) = \frac{Cov(X, Y)}{\sigma_X \cdot \sigma_Y} = \frac{\mathrm{E}[(X - \mathrm{E}X)(Y - \mathrm{E}Y)]}{\sqrt{\mathrm{E}[X - \mathrm{E}X]^2}\sqrt{\mathrm{E}[Y - \mathrm{E}Y]^2}}\]</span></p><p>对于相关系数，我们需要注意：</p><ol type="1"><li>相关系数是用于描述两个变量<em>线性</em>相关程度的，如果<span class="math inline">\(r \gt 0\)</span>，呈正相关；如果<span class="math inline">\(r = 0\)</span>，不相关；如果<span class="math inline">\(r \lt 0\)</span>，呈负相关。</li><li>如果我们将<span class="math inline">\(X - \mathrm{E}X\)</span>和<span class="math inline">\(Y - \mathrm{E}Y\)</span>看成两个向量的话，那<span class="math inline">\(r\)</span>刚好表示的是这两个向量夹角的余弦值，这也就解释了为什么<span class="math inline">\(r\)</span>的值域是[-1, 1]。</li><li>相关系数对变量的平移和缩放（线性变换）保持不变（Correlation is invariant to scaling and shift，不知道中文该如何准确表达，😅）。比如<span class="math inline">\(\mathrm{Corr}(X, Y) = \mathrm{Corr}(aX + b, Y)\)</span>恒成立。</li></ol><h2 id="决定系数r方">决定系数（R方）</h2><p>下面来说决定系数，R方一般用在回归模型用用于评估预测值和实际值的符合程度，R方的定义如下：</p><p><span class="math display">\[R^2 = 1 - \mathrm{FVU} = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} = 1 - \frac{\sum\limits_i(y_i - f_i)^2}{\sum\limits_i(y_i - \hat{y})^2}\]</span></p><p>上式中<span class="math inline">\(y\)</span>是实际值，<span class="math inline">\(f\)</span>是预测值，<span class="math inline">\(\hat{y}\)</span>是实际值的平均值。<span class="math inline">\(\mathrm{FVU}\)</span>被称为fraction of variance unexplained，RSS叫做Residual sum of squares，TSS叫做Total sum of squares。根据<span class="math inline">\(R^2\)</span>的定义，可以看到<span class="math inline">\(R^2\)</span>是有可能小于0的，所以<span class="math inline">\(R2\)</span>不是<span class="math inline">\(r\)</span>的平方。一般地，<span class="math inline">\(R^2\)</span>越接近1，表示回归分析中自变量对因变量的解释越好。</p><p>对于<span class="math inline">\(R^2\)</span>可以通俗地理解为使用均值作为误差基准，看预测误差是否大于或者小于均值基准误差。</p><p>此外，我们做这样一个变形：<span class="math inline">\(R^2 = 1 - \frac{\sum\limits_i(y_i - f_i)^2 / n}{\sum\limits_i(y_i - \hat{y})^2 / n} = 1 - \frac{\mathrm{RMSE}}{\mathrm{Var}}\)</span>，可以看到变成了1减去均方根误差和方差的比值（有利于编程实现）。</p><p>另外有一个叫做Explained sum of squares，<span class="math inline">\(\mathrm{ESS} = \sum\limits_i(f_i - \hat{y})^2\)</span></p><p>在一般地线性回归模型中，有<span class="math inline">\(\mathrm{ESS} + \mathrm{RSS} = \mathrm{TSS}\)</span>（证明过程参见：<a href="https://en.wikipedia.org/wiki/Explained_sum_of_squares#Partitioning_in_the_general_ordinary_least_squares_model" target="_blank" rel="noopener">Partitioning in the general ordinary least squares model</a>）</p><p>在这种情况下：我们有<span class="math inline">\(R^2 = 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}} = \frac{\mathrm{ESS}}{\mathrm{TSS}} = \frac{\sum\limits_i(f_i - \hat{y})^2}{\sum\limits_i(y_i - \hat{y})^2}\)</span></p><p>对于<span class="math inline">\(R^2\)</span>我们需要注意：</p><ol type="1"><li><p><span class="math inline">\(R^2\)</span>一般用在线性模型中（虽然非线性模型总也可以用），具体参见：<a href="http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit" target="_blank" rel="noopener">Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?</a></p></li><li><p><span class="math inline">\(R^2\)</span>不能完全反映模型预测能力的高低</p></li></ol><p>最后，这篇文章《<a href="https://www.displayr.com/8-tips-for-interpreting-r-squared/" target="_blank" rel="noopener">8 Tips for Interpreting R-Squared</a>》里面指出了不错误解读<span class="math inline">\(R^2\)</span>的地方，读完之后，我觉得以后还是少用<span class="math inline">\(R^2\)</span>，对于模型的评估可以选择其它一些更适合的指标。</p><h2 id="参考资料">参考资料</h2><p>[1]. <a href="http://danshiebler.com/2017-06-25-metrics/" target="_blank" rel="noopener">The relationship between correlation and the coefficient of determination</a></p><p>[2]. <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" target="_blank" rel="noopener">Coefficient of determination</a></p><p>[3]. <a href="https://en.wikipedia.org/wiki/Explained_sum_of_squares" target="_blank" rel="noopener">Explained sum of squares</a></p><p>[4]. <a href="http://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit" target="_blank" rel="noopener">Regression Analysis: How Do I Interpret R-squared and Assess the Goodness-of-Fit?</a></p><p>[5]. <a href="https://www.displayr.com/8-tips-for-interpreting-r-squared/" target="_blank" rel="noopener">8 Tips for Interpreting R-Squared</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;相关系数r和决定系数r2的那些事&quot;&gt;相关系数&lt;span class=&quot;math inline&quot;&gt;\(r\)&lt;/span&gt;和决定系数&lt;span class=&quot;math inline&quot;&gt;\(R^2\)&lt;/span&gt;的那些事&lt;/h1&gt;
&lt;p&gt;有人说相关系数（correl
      
    
    </summary>
    
      <category term="数学" scheme="http://theonegis.github.io/categories/math/"/>
    
    
      <category term="相关系数" scheme="http://theonegis.github.io/tags/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"/>
    
      <category term="决定系数" scheme="http://theonegis.github.io/tags/%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>假设检验和P值那些事</title>
    <link href="http://theonegis.github.io/math/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C%E5%92%8CP%E5%80%BC%E9%82%A3%E4%BA%9B%E4%BA%8B/"/>
    <id>http://theonegis.github.io/math/假设检验和P值那些事/</id>
    <published>2019-01-04T08:17:52.000Z</published>
    <updated>2019-03-22T19:39:53.830Z</updated>
    
    <content type="html"><![CDATA[<h1 id="假设检验和p值那些事">假设检验和P值那些事</h1><p>记得大学时候学习概率论与数理统计的时候，学习过假设检验，但我不记得课本上有提到过P值。后来翻阅了一些资料，大概弄明白了它们之间的关系，本文旨在以浅显易懂的语言描述严密的数学知识。</p><h2 id="假设检验">假设检验</h2><p>在《Head First Statistics》一书中，作者给假设检验的定义是“Hypothesis tests give you a way of using samples to test whether or not statistical claims are likely to be true”。其实定义不重要，重要的是我们需要知道假设检验能做什么：以概率统计的视角判别一个统计假说是否成立。</p><p>下面举一个烂大街的例子：我有一枚专门用于玩抛硬币猜正反面的游戏的硬币，我需要判断这枚硬币是否是正常的（抛硬币游戏中出现正反面的概率相等）。所以我做了一个假说：该枚硬币是正常的，即抛硬币游戏中出现正面的概率为0.5。</p><p>那我现在需要做实验去验证我说的对不对。我抛了20次，正面朝上11次，背面朝上9次（设正面朝上记为1，反面朝上记为0）。基于这个实验结果，我应该做怎样的判断呢？</p><p>根据假设检验的一般步骤：</p><ol type="1"><li><p>建立假设</p></li><li><p>寻找检验统计量</p></li><li><p>确定显著性水平和拒绝域</p></li><li><p>做出判断</p></li></ol><p>第一步中我们的原假设<span class="math inline">\(H_0\)</span>（null hypothesis）为该枚硬币是正常的，备择假设<span class="math inline">\(H_1\)</span>（alternate hypothesis）为该硬币不正常。</p><p>注：当原假设正确，而由于样本的随机性使得样本观测值落在拒绝域（critical region或rejection region）而拒绝原假设产生的错误称为第一类错误；当原假设错误，而样本观测值落在接受域而接受原假设产生的错误称为第二类错误。</p><p>第二步中根据中心极限定理可知随机变量<span class="math inline">\(\bar{X}\)</span>服从正态分布。这里我们的检验统计量选择<span class="math inline">\(t=\frac{\bar{X}-\mu_0}{S/\sqrt{n}}\)</span>（这里的<span class="math inline">\(t\)</span>服从自由度为<span class="math inline">\(n-1\)</span>的<span class="math inline">\(t\)</span>分布），所以我们使用<span class="math inline">\(t\)</span>分布来估计投掷的均值（这里<span class="math inline">\(\bar{X}\)</span>为样本均值，<span class="math inline">\(\mu_0\)</span>为原假设中的均值（期望），<span class="math inline">\(S\)</span>为样本标准差，<span class="math inline">\(n\)</span>为样本个数）。</p><p><span class="math display">\[t=\frac{\bar{X}-\mu_0}{S/\sqrt{n}} = \frac{0.55 - 0.5}{0.5104178 / \sqrt{20}} = 0.4380858\]</span></p><p>注：<span class="math inline">\(t\)</span>分布用于根据小样本来估计呈正态分布且方差未知的总体的均值，称为<span class="math inline">\(t\)</span>检验。如果总体方差已知（例如在样本数量足够多时），则应该用正态分布来估计总体均值，称为<span class="math inline">\(U\)</span>检验。</p><p>第三步中显著性水平<span class="math inline">\(\alpha\)</span>（significance level，拒绝原假设时概率阈值）我们一般采用0.05（当然，你也可以使用0.1或者其它）。这个0.05的意思是观测值落在拒绝域的概率为0.05，概率为0.05说明这是小概率事件，而在一次测试中发生了小概率事件，所以我们有足够的理由拒绝原假设。</p><p>接下来我们应该计算拒绝域了。对于<span class="math inline">\(t\)</span>分布求0.025和0.975的分位数分别为-2.093024和2.093024（即<span class="math inline">\(t\)</span>的上下界，左右两边各是0.025，合起来就是0.05的拒绝域），我们可以反推出<span class="math inline">\(\bar{X}\)</span>的上下界为0.3111171和0.7888829（这个区间就是接受域）。</p><p>注：对于拒绝域来说，有单边和双边情况，我们这里显然是双边的情况。</p><p>第四步做出判断，我们实验的结果的均值是0.4380858，我们在0.05的显著性水平下得到的接受域是<span class="math inline">\((0.3111171, 0.7888829)\)</span>，实验结果落在接受域，所以我们不能拒绝原假设<span class="math inline">\(H_0\)</span>。这里的不能拒绝指的是我们没有足够的理由推翻原假设，但是这并不代表原假设一定正确。</p><h2 id="p值">P值</h2><p>上面讲了检验假设的一般过程，好像跟P值没什么关系？但是P值其实和检验假设息息相关的。上面的求解过程是通过判断样本观测值是否落在拒绝域而做出判断的，其实我们还可以通过计算P值直接进行判断。</p><p>那么什么是<span class="math inline">\(P\)</span>值呢？《Head First Statistics》给出的定义是“A p-value is the probability of getting the results in the sample, or something more extreme, in the direction of the critical region.”。我的理解就是P值是在原假设成立的情况下，出现比当前样本观测值更极端（包括当前样本观测值）情况的概率。</p><p>其实这样说还是挺抽象的，我们通过计算来进行说明。</p><p>我们把检验假设步骤中的第三步修改为：确定显著性水平和计算P值</p><p>在我们的实验中<span class="math inline">\(t=\frac{\bar{X}-\mu_0}{S/\sqrt{n}} = \frac{0.55 - 0.5}{0.5104178 / \sqrt{20}} = 0.4380858\)</span></p><p>然后我们通过查表可以得到0.4380858对应的上侧分位数为 0.3331321（和-0.4380858对应的下侧分位数相同），因为我们的实验中是双边情况，所以<span class="math inline">\(P = 0.4380858\times2 = 0.6662642 \gt 0.05\)</span></p><p>在确定了显著性水平<span class="math inline">\(\alpha\)</span>的情况下（<span class="math inline">\(\alpha=0.05\)</span>），如果计算出的<span class="math inline">\(P\le0.05\)</span>，说明观察值不合理，也就是样本均值离假设均值太远了，因此拒绝原假如果计算计算出的<span class="math inline">\(P\gt0.05\)</span>，则我们不能拒绝原假设。</p><p>注：设连续型随机变量<span class="math inline">\(X\)</span>的分布函数为<span class="math inline">\(F(x)\)</span>，密度函数为<span class="math inline">\(f(x)\)</span>，对于任意<span class="math inline">\(\alpha (0\lt\alpha\lt1)\)</span>，假如<span class="math inline">\(x_\alpha\)</span>满足条件</p><p><span class="math display">\[F(x_\alpha) = \int_{-\infty}^{x_\alpha} f(x) dx = \alpha\]</span></p><p>则<span class="math inline">\(x_\alpha\)</span>称为<span class="math inline">\(X\)</span>分布的<span class="math inline">\(\alpha\)</span>分位数，或称为<span class="math inline">\(\alpha\)</span>下侧分位数。假如<span class="math inline">\(x^{\prime}_\alpha\)</span>满足</p><p><span class="math display">\[1 - F(x^{\prime}_\alpha) = \int_{-\infty}^{x_\alpha} f(x) dx = \alpha\]</span></p><p>则<span class="math inline">\(x^{\prime}_\alpha\)</span>称为<span class="math inline">\(X\)</span>分布的<span class="math inline">\(\alpha\)</span>分位数。</p><p>通俗地理解分位数就是对应某个概率面积的横坐标，如果是左侧面积（概率）叫下侧分位数，如果是右侧面积（概率）叫上侧分位数。</p><h2 id="r中的实践">R中的实践</h2><p>好的，下面我们来看如何在R中重复上面的实验：</p><p>产生一个随机的模拟序列（二项分布，生成0和1）</p><p><code>flips &lt;- rbinom(20, 1, 0.4)</code></p><p>结果如下：<code>1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1</code></p><p>使用R内置的<span class="math inline">\(t\)</span>检验函数如下：</p><p><code>t.test (flips, mu=0.5)</code></p><p>输出结果如下：</p><pre><code>    One Sample t-testdata:  flipst = 0.43809, df = 19, p-value = 0.6663alternative hypothesis: true mean is not equal to 0.595 percent confidence interval: 0.3111171 0.7888829sample estimates:mean of x      0.55 </code></pre><p>如果需要我们手动计算上面这些值，我们是否可以计算出来呢？计算的过程就是上面讲解假设减压和P值的过程。</p><p>首先<span class="math inline">\(t\)</span>值的计算很简单，使用公式<span class="math inline">\(t=\frac{\bar{X}-\mu_0}{S/\sqrt{n}}\)</span>即可，代码如下，结果为0.4380858</p><p><code>(mean(flips) - 0.5) / (sd(flips) / sqrt(length(flips)))</code></p><p><span class="math inline">\(df\)</span>指代的是自由度<span class="math inline">\(n-1=20-1=19\)</span></p><p>接下来我们计算P值，通过计算-0.4380858的下侧分位数再乘以2可得，代码如下，结果为0.6662642</p><p><code>pt(-0.4380858, 19) * 2</code></p><p>在来看我们的置信区间，其实这里就是我们所要求的接受域</p><p>左侧计算代码：<code>qt(0.025, 19) * (sd(flips) / sqrt(20)) + 0.55</code> 结果为：0.3111171</p><p>右侧计算代码：<code>qt(0.975, 19) * (sd(flips) / sqrt(20)) + 0.55</code> 结果为：0.7888829</p><h2 id="参考文献">参考文献</h2><p>[1] 茆诗松. 概率论与数理统计 (第二版)[M]. 2000.</p><p>[2] Griffiths D. Head first statistics[M]. Oreilly Vlg Gmbh &amp; Co, 2009.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;假设检验和p值那些事&quot;&gt;假设检验和P值那些事&lt;/h1&gt;
&lt;p&gt;记得大学时候学习概率论与数理统计的时候，学习过假设检验，但我不记得课本上有提到过P值。后来翻阅了一些资料，大概弄明白了它们之间的关系，本文旨在以浅显易懂的语言描述严密的数学知识。&lt;/p&gt;
&lt;h2 id=
      
    
    </summary>
    
      <category term="数学" scheme="http://theonegis.github.io/categories/math/"/>
    
    
      <category term="假设检验" scheme="http://theonegis.github.io/tags/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/"/>
    
      <category term="P值" scheme="http://theonegis.github.io/tags/P%E5%80%BC/"/>
    
  </entry>
  
  <entry>
    <title>使用Python计算方差协方差相关系数</title>
    <link href="http://theonegis.github.io/math/%E4%BD%BF%E7%94%A8Python%E8%AE%A1%E7%AE%97%E6%96%B9%E5%B7%AE%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"/>
    <id>http://theonegis.github.io/math/使用Python计算方差协方差相关系数/</id>
    <published>2018-12-17T03:32:10.000Z</published>
    <updated>2019-03-22T19:39:53.828Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用python计算方差协方差和相关系数">使用Python计算方差，协方差和相关系数</h1><p>[TOC]</p><h2 id="数学定义">数学定义</h2><h3 id="期望">期望</h3><p>设随机变量<span class="math inline">\(X\)</span>只取有限个可能值<span class="math inline">\(a_i (i=0, 1, ..., m)\)</span>，其概率分布为<span class="math inline">\(P (X = a_i) = p_i\)</span>. 则<span class="math inline">\(X\)</span>的数学期望，记为<span class="math inline">\(E(X)\)</span>或<span class="math inline">\(EX\)</span>，定义为：</p><p><span class="math display">\[E(X) = \sum\limits_ia_ip_i\]</span></p><h3 id="方差">方差</h3><p>设<span class="math inline">\(X\)</span>为随机变量，分布为<span class="math inline">\(F\)</span>，则</p><p><span class="math display">\[Var(X) = E(X-EX)^2 \]</span></p><p>称为<span class="math inline">\(X\)</span>(或分布<span class="math inline">\(F\)</span>)的方差，其平方根<span class="math inline">\(\sqrt{Var(X)}\)</span>称为<span class="math inline">\(X\)</span>(或分布<span class="math inline">\(F\)</span>)的标准差.</p><p>方差和标准差是刻画随机变量在其中心位置附近散布程度的数字特征。</p><p>注意：样本方差和总体方差的区别</p><p>统计学上对于样本方差的无偏估计使用如下公式计算：</p><p><span class="math display">\[s^2 = \frac{1}{n-1} \sum\limits_{i=1}^n(x_i -\bar{x})^2 \]</span></p><p>前面有一个系数<span class="math inline">\(\frac{1}{n-1}\)</span>，当时当样本数量很大的时候，<span class="math inline">\(\frac{n}{n-1}\)</span>近似为1，可以直接使用总体方差公式进行计算。</p><h3 id="协方差">协方差</h3><p>协方差用来刻画两个随机变量<span class="math inline">\(X, Y\)</span>之间的相关性，定义为</p><p><span class="math display">\[Cov(X, Y) = E[(X - EX)(Y-EY)]\]</span></p><p>如果协方差为正，说明X，Y同向变化，协方差越大说明同向程度越高；如果协方差为负，说明X，Y反向运动，协方差越小说明反向程度越高</p><h3 id="相关系数">相关系数</h3><p>相关系数可以理解为标准化以后的协方差，设<span class="math inline">\(X\)</span>的标准差为<span class="math inline">\(\sigma_x\)</span>，<span class="math inline">\(Y\)</span>的标准差为<span class="math inline">\(\sigma_y\)</span>定义为</p><p><span class="math display">\[\rho = \frac{Cov(X, Y)}{\sigma_x\sigma_y}\]</span></p><p>相关系数消除了两个变量变化幅度的影响，而只是单纯反应两个变量每单位变化时的相似程度</p><h3 id="协方差矩阵">协方差矩阵</h3><p>协方差只能表示两个随机变量的相关程度（二维问题），对于大于二维的随机变量，可以使用协方差矩阵表示.</p><p>协方差矩阵的每一个值就是对应下标的两个随机变量的协方差</p><p>对于三维协方差矩阵，<span class="math inline">\(C=\begin{bmatrix}Cov(X, X) &amp; Cov(X, Y) &amp; Cov(X, Z) \\ Cov(Y, X) &amp; Cov(Y, Y) &amp; Cov(X, Y) \\ Cov(Z, X) &amp; Cov(Z, Y) &amp; Cov(Z, Z)\end{bmatrix}\)</span></p><h2 id="使用numpy包计算">使用NumPy包计算</h2><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb1-2" data-line-number="2"></a><a class="sourceLine" id="cb1-3" data-line-number="3"><span class="co"># 随机生成两个样本</span></a><a class="sourceLine" id="cb1-4" data-line-number="4">x <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">1000</span>)</a><a class="sourceLine" id="cb1-5" data-line-number="5">y <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">9</span>, <span class="dv">1000</span>)</a><a class="sourceLine" id="cb1-6" data-line-number="6"></a><a class="sourceLine" id="cb1-7" data-line-number="7"><span class="co"># 计算平均值</span></a><a class="sourceLine" id="cb1-8" data-line-number="8">mx <span class="op">=</span> x.mean()</a><a class="sourceLine" id="cb1-9" data-line-number="9">my <span class="op">=</span> y.mean()</a><a class="sourceLine" id="cb1-10" data-line-number="10"></a><a class="sourceLine" id="cb1-11" data-line-number="11"><span class="co"># 计算标准差</span></a><a class="sourceLine" id="cb1-12" data-line-number="12">stdx <span class="op">=</span> x.std()</a><a class="sourceLine" id="cb1-13" data-line-number="13">stdy <span class="op">=</span> y.std()</a><a class="sourceLine" id="cb1-14" data-line-number="14"></a><a class="sourceLine" id="cb1-15" data-line-number="15"><span class="co"># 计算协方差矩阵</span></a><a class="sourceLine" id="cb1-16" data-line-number="16">covxy <span class="op">=</span> np.cov(x, y)</a><a class="sourceLine" id="cb1-17" data-line-number="17"><span class="bu">print</span>(covxy)</a><a class="sourceLine" id="cb1-18" data-line-number="18"></a><a class="sourceLine" id="cb1-19" data-line-number="19"><span class="co"># 我们可以手动进行验证</span></a><a class="sourceLine" id="cb1-20" data-line-number="20"><span class="co"># covx等于covxy[0, 0], covy等于covxy[1, 1]</span></a><a class="sourceLine" id="cb1-21" data-line-number="21"><span class="co"># 我们这里的计算结果应该是约等于，因为我们在计算的时候是使用的总体方差(总体方差和样本方差是稍微有点区别的)</span></a><a class="sourceLine" id="cb1-22" data-line-number="22">covx <span class="op">=</span> np.mean((x <span class="op">-</span> x.mean()) <span class="op">**</span> <span class="dv">2</span>) </a><a class="sourceLine" id="cb1-23" data-line-number="23">covy <span class="op">=</span> np.mean((y <span class="op">-</span> y.mean()) <span class="op">**</span> <span class="dv">2</span>) </a><a class="sourceLine" id="cb1-24" data-line-number="24"><span class="bu">print</span>(covx)</a><a class="sourceLine" id="cb1-25" data-line-number="25"><span class="bu">print</span>(covy)</a><a class="sourceLine" id="cb1-26" data-line-number="26"><span class="co"># 这里计算的covxy等于上面的covxy[0, 1]和covxy[1, 0]，三者相等</span></a><a class="sourceLine" id="cb1-27" data-line-number="27">covxy <span class="op">=</span> np.mean((x <span class="op">-</span> x.mean()) <span class="op">*</span> (y <span class="op">-</span> y.mean()))</a><a class="sourceLine" id="cb1-28" data-line-number="28"><span class="bu">print</span>(covxy)</a><a class="sourceLine" id="cb1-29" data-line-number="29"></a><a class="sourceLine" id="cb1-30" data-line-number="30"><span class="co"># 下面计算的是相关系数矩阵(和上面的协方差矩阵是类似的)</span></a><a class="sourceLine" id="cb1-31" data-line-number="31">coefxy <span class="op">=</span> np.corrcoef(x, y)</a><a class="sourceLine" id="cb1-32" data-line-number="32"><span class="bu">print</span>(coefxy)</a></code></pre></div><p>一组可能的输出结果：</p><pre><code>[[6.83907508 0.10925926] [0.10925926 6.53390891]]6.8322366.5273750.10914999999999989[[1.         0.01634455] [0.01634455 1.        ]]</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;使用python计算方差协方差和相关系数&quot;&gt;使用Python计算方差，协方差和相关系数&lt;/h1&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;数学定义&quot;&gt;数学定义&lt;/h2&gt;
&lt;h3 id=&quot;期望&quot;&gt;期望&lt;/h3&gt;
&lt;p&gt;设随机变量&lt;span class=&quot;math 
      
    
    </summary>
    
      <category term="数学" scheme="http://theonegis.github.io/categories/math/"/>
    
    
      <category term="数理统计" scheme="http://theonegis.github.io/tags/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
    
      <category term="方差" scheme="http://theonegis.github.io/tags/%E6%96%B9%E5%B7%AE/"/>
    
      <category term="协方差" scheme="http://theonegis.github.io/tags/%E5%8D%8F%E6%96%B9%E5%B7%AE/"/>
    
      <category term="相关系数" scheme="http://theonegis.github.io/tags/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>一文详解卷积和逆卷积</title>
    <link href="http://theonegis.github.io/dl/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3%E5%8D%B7%E7%A7%AF%E5%92%8C%E9%80%86%E5%8D%B7%E7%A7%AF/"/>
    <id>http://theonegis.github.io/dl/一文详解卷积和逆卷积/</id>
    <published>2018-11-09T06:52:57.000Z</published>
    <updated>2019-03-22T19:39:53.827Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一文详解卷积和逆卷积">一文详解卷积和逆卷积</h1><p>卷积神经网络（CNN）在计算机视觉大放异彩，入门CNN的第一步就是理解什么是卷积（Convolution）运算。本文旨在以通俗易懂的方式让读者理解卷积的概念。</p><p>注：本文的图片素材全部来源于网络，如有侵权，请联系作者删除。</p><h2 id="卷积运算">卷积运算</h2><p>卷积在数学上是两个变量在某范围内相乘后求和的结果。在数字图像处理中，卷积操作其实就是利用卷积核（卷积模板）在图像上滑动，将图像点上的像素灰度值与对应的卷积核上的数值相乘，然后将所有相乘后的值相加作为卷积核中间像素对应的图像上像素的灰度值，并最终滑动完所有图像的过程，如图下图所示（图中的蓝色代表输入，青色代表输出）。</p><figure><img src="/images/ml/no_padding_no_strides.gif" alt="卷积运算"><figcaption>卷积运算</figcaption></figure><p>在卷积运算中，我们有下面几个概念：</p><ul><li>Padding 有的时候对于给定输入图像的大小，我们需要得到制定大小的输出。这时候，我们可以通过给图像的边缘增加0像素值得方法获得。这个0像素值区域的大小，我们一般称之为padding。</li><li>Stride 在一般的卷积过程中，我们使用步长为1进行卷积核在图像上的滑动；但是有时候出于缩小输出图片尺寸的原因，我们也会采用大于1的步长。卷积核每次滑动的步长，我们称之为stride。</li></ul><table><thead><tr class="header"><th style="text-align: center;"><img src="/images/ml/same_padding_no_strides.gif" alt="Padding为1的卷积" width="260"></th><th style="text-align: center;"><img src="/images/ml/no_padding_strides.gif" alt="Stride为2的卷积" width="300"></th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Padding为<span class="math inline">\(1\times1\)</span>的卷积操作</td><td style="text-align: center;">Stride为<span class="math inline">\(2\times2\)</span>的卷积操作</td></tr></tbody></table><h3 id="单通道">单通道</h3><p>对于一个长为<span class="math inline">\(H\)</span>，宽为<span class="math inline">\(W\)</span>的灰度图像来说，其尺寸可以表示为<span class="math inline">\((1, W, H)\)</span>。这里的1我们称之为通道（Channel）。所谓“一图胜千言”，下面的动图以单通道为例，演示了如何进行卷积运算得到最终的输出结果。</p><figure><img src="/images/ml/stride1.gif" alt="步长为1的卷积"><figcaption>步长为1的卷积</figcaption></figure><figure><img src="/images/ml/stride2.gif" alt="步长为2的卷积"><figcaption>步长为2的卷积</figcaption></figure><h3 id="多通道">多通道</h3><p>那如果对于一个RGB彩色图像我们的通道就是3，对于多通道的输入，我们如何进行卷积操作，输出为多通道或者单通道呢？</p><figure><img src="/images/ml/rgb.gif" alt="多通道卷积"><figcaption>多通道卷积</figcaption></figure><p>从上图可以知道，对于多通道的输入，卷积操作在每个Channel上分别进行，然后进行求和得到输出。比如，我的输入是<span class="math inline">\((32, W, H)\)</span>，我的输出是<span class="math inline">\((64, W, H)\)</span>，则需要新的卷积核的个数是<span class="math inline">\(32 \times 64\)</span>。因为，对于在输入的32个通道的每个通道都需要64个卷积核，每个通道做完卷积运算，然后再求和，得到最后的64个通道的输出。</p><h3 id="卷积运算的参数计算">卷积运算的参数计算</h3><p>根据前面的分析，到这里，卷积运算的参数的求解就很明确了。</p><p>设我们的输入通道是<span class="math inline">\(p\)</span>，输出通道是<span class="math inline">\(q\)</span>，则</p><p>如果我们不考虑Bias（增益），那么对于一个<span class="math inline">\(m \times n\)</span>的卷积核（我们一般取<span class="math inline">\(m =n\)</span>），我们需要学习的参数为<span class="math inline">\((m \times n \times p) \times q\)</span>；</p><p>如果加上Bias（就是给卷积核作用以后的结果添加一个常数），那么我们需要学习的参数为$(m n p + 1) q $。</p><h2 id="逆卷积">逆卷积</h2><p>在CNN中，我们经常会使用所谓的逆卷积（Decovolution）进行输入尺寸的放大，但是注意这个逆卷积不是卷积的逆操作。下面，我们还是看图说话，到底什么是逆卷积呢？(图中蓝色代表的是输入，青色代表的是输出)</p><figure><img src="/images/ml/no_padding_no_strides_transposed.gif" alt="逆卷积"><figcaption>逆卷积</figcaption></figure><p>可以看到其实逆卷积和卷积操作并没有本质的区别，只是在输出的尺寸上有所区别。</p><h2 id="卷积运算的矩阵实现">卷积运算的矩阵实现</h2><p>那么在计算机内部我们如何实现卷积操作呢？答案是矩阵乘法。</p><p>我们还是看图说话，对于卷积操作，我们对输入图像以及卷积核做Unroll操作以后，进行矩阵相乘得到输出。</p><figure><img src="/images/ml/deconvolution-unroll.jpg" alt="卷积操作的矩阵实现"><figcaption>卷积操作的矩阵实现</figcaption></figure><p>对于逆卷积操作，我们对卷积核进行Unroll操作以后进行转置，然后再做矩阵乘法，得到输入。所以基于这个原因，我们一般称逆卷积为转置卷积（Transposed Convolution）。</p><figure><img src="/images/ml/deconvolution.jpg" alt="逆卷积操作的矩阵实现"><figcaption>逆卷积操作的矩阵实现</figcaption></figure><h2 id="参考资料">参考资料</h2><ol type="1"><li><a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" target="_blank" rel="noopener">Convolution arithmetic</a></li><li><a href="http://machinelearninguru.com/computer_vision/basics/convolution/convolution_layer.html" target="_blank" rel="noopener">Undrestanding Convolutional Layers in Convolutional Neural Networks</a></li><li><a href="https://nrupatunga.github.io/convolution-2/" target="_blank" rel="noopener">Convolution Arithmetic in Deep Learning</a></li><li><a href="https://arxiv.org/pdf/1603.07285.pdf" target="_blank" rel="noopener">A guide to convolution arithmetic for deep learning</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;一文详解卷积和逆卷积&quot;&gt;一文详解卷积和逆卷积&lt;/h1&gt;
&lt;p&gt;卷积神经网络（CNN）在计算机视觉大放异彩，入门CNN的第一步就是理解什么是卷积（Convolution）运算。本文旨在以通俗易懂的方式让读者理解卷积的概念。&lt;/p&gt;
&lt;p&gt;注：本文的图片素材全部来源于
      
    
    </summary>
    
      <category term="深度学习" scheme="http://theonegis.github.io/categories/dl/"/>
    
    
      <category term="深度学习" scheme="http://theonegis.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积" scheme="http://theonegis.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="转置卷积" scheme="http://theonegis.github.io/tags/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="逆卷积" scheme="http://theonegis.github.io/tags/%E9%80%86%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
</feed>
